\section{Results and Testing}
\label{sec:results-and-testing}
This chapter will focus on the results of the project, and the testing that was carried out to evaluate the performance of the different subsystems of the project. The chapter will be divided into three sections: Mechanical Design, Electronics and Software, and Computer Vision.

\subsection{Mechanical Design}

\subsection{Electronics and Software}

\subsection{Computer Vision}
\label{sec:computer-vision-evaluation}
This section will examine the Computer Vision system, as discussed in \autoref{sec:computer-vision-system} and implemented in \autoref{sec:computer-vision}.

\subsubsection{Component Detection Model}
The Component Detection model was trained on a dataset of 1076 images, split into a 70-20-10 split for training, validation, and testing respectively. The dataset was augmented using the augmentations discussed in \autoref{sec:image-processing}. The model was trained using the training parameters shown in \autoref{tab:training-parameters}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{imgs/articles/iou.png}
    \caption{IoU Example \cite{rosebrock_2016}}
    \label{fig:iou}
  \end{figure}
  
As discussed in \autoref{sec:real-time-object-detection}, mAP\raisebox{-1pt}{\textsuperscript{50}} is a standard metric used to evaluate object detection models. mAP\raisebox{-1pt}{\textsuperscript{50-95}} is a more rigorous metric that aggregates the mean average precision across all classes at different Intersection over Union (IoU) thresholds; in mAP\raisebox{-1pt}{\textsuperscript{50}}, an object is considered as detected if the IoU is greater than 0.5 (the predicted bounding box overlaps the ground truth bounding box by at least 50\%), whereas in mAP\raisebox{-1pt}{\textsuperscript{50-95}}, the mAP score is aggregated across different IoU thresholds from 0.5 to 0.95 in steps of 0.05. This can be seen more clearly \autoref{fig:iou}.
  
\begin{figure}[H]
  \centering
  \includesvg[width=0.8\textwidth]{imgs/graphs/metrics_mAP50-95(B).svg} 
  \caption{TensorBoard graphing: all runs mAP\raisebox{-1pt}{\textsuperscript{50-95}} against epochs}
  \label{fig:metrics}
\end{figure}
  
In \autoref{fig:metrics}, the mAP\raisebox{-1pt}{\textsuperscript{50-95}} is shown for all training runs of the model with slightly different training parameters. It can be seen that the models tend to converge around 20 epochs, and there is a ceiling at around 80\% mAP\raisebox{-1pt}{\textsuperscript{50-95}}. This is likely due to the small dataset size, and it is made even smaller due to the use of a test set. For deployment, the training set will absorb the test set to increase the size of the dataset and improve the model's performance, however for discussion it is necessary to keep a test set separate to objectively measure the model's performance.


\begin{figure}[H]
  \centering
  \begin{minipage}[t]{0.8\textwidth}
    \centering
    \includesvg[width=\textwidth]{imgs/graphs/finalmodelmap95.svg}
  \end{minipage}
  \par\medskip
  \begin{minipage}[t]{0.8\textwidth}
    \centering
    \includesvg[width=\textwidth]{imgs/graphs/metrics_mAP50(B).svg}
  \end{minipage}
  \caption{Final Model (green) and non-pretrained model (orange) mAP\raisebox{-1pt}{\textsuperscript{50-95}} and mAP\raisebox{-1pt}{\textsuperscript{50}} scores}
  \label{fig:final-model-metrics}
\end{figure}

As shown in \autoref{fig:final-model-metrics}, the final YOLO-OBB model (green) achieved an mAP\raisebox{-1pt}{\textsuperscript{50-95}} of 81.1\% and an incredibly high mAP\raisebox{-1pt}{\textsuperscript{50}} of 99.5\%. This is an extremely impressive score given the relatively small size of the dataset (and made even smaller due to the need of the test set), which is truly a testament to YOLOv8's capabilities. An experiment was run to see how the model would perform if the model was \textbf{not} pretrained on DOTOv1 \cite{9560031} given the low training time to see how much of an impact the pretraining had on the model's performance. 

The model, shown in orange, clearly takes much longer to converge; where the original model reaches 99.5\% mAP\raisebox{-1pt}{\textsuperscript{50}} for the first time in 14 epochs, the non-pretrained model had an mAP\raisebox{-1pt}{\textsuperscript{50}} of 82.5\%, and takes 40 epochs to reach 98\%. By the time the pretrained model finishes training at 26 epochs at an mAP\raisebox{-1pt}{\textsuperscript{50-95}} of 81.1\%, the non-pretrained model had an mAP\raisebox{-1pt}{\textsuperscript{50}} of 67.4\%, a substantial difference. This clearly demonstrates the importance of pretraining on a large dataset to improve the model's performance and training time. When the model is pretrained on a large dataset, the model is able to learn generalisable, basic features such as edge and corner detection, which are common in most images. When the model is now trained on a new dataset, it does not need to relearn these basic features, and can instead focus on learning the more complex features of the new dataset. This is known as transfer learning, and it is a common technique used in deep learning to improve the performance of models on new datasets.

\begin{figure}[H]
  \centering
  \includesvg[width=0.8\textwidth]{imgs/graphs/all.svg}
  \caption{All training runs mAP\raisebox{-1pt}{\textsuperscript{50-95}} against epochs}
  \label{fig:allruns}
\end{figure}

For completeness, the mAP\raisebox{-1pt}{\textsuperscript{50-95}} for all training runs is shown in \autoref{fig:allruns}, where the orange non-pretrained model is clearly lagging behind all other training runs.

To properly evaluate a model's performance, it is important to consider the model's performance on a test set. As described above, 10\% of the dataset was split for this, giving 98 



In \autoref{fig:confusion-matrix}, the confusion matrix for the test set is shown. 


% The model achieved an mAP\raisebox{-1pt}{\textsuperscript{50-95}} of 77.8\% and an AP\raisebox{-1pt}{\textsuperscript{50}} of 99.5\%, which is only a 3.3\% drop in mAP\raisebox{-1pt}{\textsuperscript{50-95}}. This is a very small drop in performance, and it was decided that the pretraining was worth it given the small drop in performance and the increase in training time. The model was therefore trained with the parameters shown in \autoref{tab:training-parameters}.

  
  \todo{talk about tensor board}
  \todo{show metrics}
  
On the GTX 3080 Ti, 
  
Interestingly, if Class Loss is set to 1.0, Box Loss is set to 4.0, and DFL is set to 1.5, the model converges in around ~15 epochs, taking only ~2 minutes to train. However, the model's performance is slightly impacted, calculating metrics on the test set yields an mAP\raisebox{-1pt}{\textsuperscript{50-95}} of 77.8\%, but it was decided that the time savings were not worth it given it was only a few minutes of training time. The model was therefore trained with the parameters shown in \autoref{tab:training-parameters}.

\todo{show pictures of model in action}

\subsubsection{Resistor Value Classification Model}

\todo{add power consumption measurements from usb for complete power - implementation maybe?}

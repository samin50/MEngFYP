{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "from os import path, listdir, makedirs\n",
    "from shutil import rmtree, copy\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the full classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "IMAGE_SIZE = 640 # x 480\n",
    "PROJECT_NAME = \"logs\"\n",
    "RECIPE_NAME = \"all\"\n",
    "TRAIN_RUN = f\"{RECIPE_NAME}_run-{time.strftime('%Y%m%d-%H%M%S')}\"\n",
    "DATA_PATH = \"./datasets/full\"\n",
    "\n",
    "# Model\n",
    "MODEL_PATH = f\"./models/pretrained/yolov8n-obb-dotav1.pt\"\n",
    "TRAIN_PARAM = {\n",
    "    # Model definition\n",
    "    'data': f\"./models/recipes/dataset_desc.yaml\",\n",
    "    'resume': False,\n",
    "    'device': '0',\n",
    "    'pretrained': True,\n",
    "    # Names\n",
    "    'project' : PROJECT_NAME,\n",
    "    'name': TRAIN_RUN,\n",
    "    # Training Parameters\n",
    "    'batch': -1,\n",
    "    'imgsz': IMAGE_SIZE,\n",
    "    'epochs': 20,\n",
    "    'patience': 10,\n",
    "    'cos_lr': True,\n",
    "    # Augmentation\n",
    "    'hsv_h': 0.05, # Higher than default for resistor\n",
    "    'hsv_s': 0.3, # Colours should not change too much\n",
    "    'hsv_v': 0.2, # Colours should not change too much\n",
    "    'degrees': 180, # Rotation\n",
    "    'translate': 0.1, # Translation\n",
    "    'scale': 0.8, # Scaling - camera is always at the same distance\n",
    "    'shear': 10.0, # Shearing\n",
    "    'perspective': 0.0, # Perspective\n",
    "    'flipud': 0.5, # Flip up-down\n",
    "    'fliplr': 0.5, # Flip left-right\n",
    "    'mosaic': 0.5, # Mosaic\n",
    "    'mixup': 0.0, # Mixup\n",
    "    'copy_paste': 0.0, # Copy-paste\n",
    "    'crop_fraction': 1.0, # Crop fraction\n",
    "    # Loss weights\n",
    "    'cls' : 1.0, # Class\n",
    "    'box' : 4.0, # Box accuracy\n",
    "    'dfl' : 1.5, # Help manage unbalanced classes\n",
    "    # Post parameters\n",
    "    'save': True,\n",
    "    'save_period': 5,\n",
    "    'plots': False,\n",
    "    # Misc\n",
    "    'verbose': False,\n",
    "}\n",
    "\n",
    "PATHS = {\n",
    "    'train': f\"{DATA_PATH}/current/images/train\",\n",
    "    'val': f\"{DATA_PATH}/current/images/val\",\n",
    "    'test': f\"{DATA_PATH}/current/images/test\",\n",
    "    'resistors' : f\"{DATA_PATH}/resistor/imgs\",\n",
    "    'ceramic_cap' : f\"{DATA_PATH}/ceramic_capacitor/imgs\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 164 images and 164 labels in capacitor\n",
      "Found 264 images and 264 labels in ceramic_capacitor\n",
      "Found 66 images and 66 labels in film_capacitor\n",
      "Found 52 images and 52 labels in inductor\n",
      "Found 96 images and 96 labels in led\n",
      "Found 258 images and 259 labels in resistor\n",
      "Found 75 images and 75 labels in wire\n",
      "Split into 682 train, 195 val, 97 test. Total: 974 images.\n"
     ]
    }
   ],
   "source": [
    "# Reorganise the data\n",
    "PARAM = {\n",
    "    'path' : DATA_PATH,\n",
    "    'train' : 0.7,\n",
    "    'val' : 0.2,\n",
    "    'test' : 0.1,\n",
    "    'include' : [\n",
    "        'resistor',\n",
    "        'capacitor',\n",
    "        'ceramic_capacitor',\n",
    "        'film_capacitor',\n",
    "        'inductor',\n",
    "        'led',\n",
    "        'wire'\n",
    "    ]\n",
    "}\n",
    "# Remove old dataset\n",
    "DATASET_FOLDER = f\"{DATA_PATH}/current\"\n",
    "if path.exists(DATASET_FOLDER):\n",
    "    for folder in listdir(DATASET_FOLDER):\n",
    "        rmtree(path.join(DATASET_FOLDER, folder), ignore_errors=True)\n",
    "    # Make label folder\n",
    "    makedirs(path.join(DATASET_FOLDER, 'labels'), exist_ok=True)\n",
    "    # Make train, val, test folders\n",
    "    for folder in ['train', 'val', 'test']:\n",
    "        makedirs(path.join(DATASET_FOLDER, 'images', folder), exist_ok=True)\n",
    "        makedirs(path.join(DATASET_FOLDER, 'labels', folder), exist_ok=True)\n",
    "\n",
    "# Get all component images from all folders\n",
    "basenames = []\n",
    "# For component folder in dataset\n",
    "for folder in listdir(PARAM['path']):\n",
    "    # Skip if not in include\n",
    "    if folder not in PARAM['include']: continue\n",
    "    # For each subfolder in component folder\n",
    "    imgfiles = listdir(path.join(PARAM['path'], folder, 'imgs'))\n",
    "    labfiles = listdir(path.join(PARAM['path'], folder, 'labels'))\n",
    "    bases = [path.join(folder, 'imgs', path.splitext(f)[0]) for f in imgfiles]\n",
    "    basenames.extend(bases)\n",
    "    print(f\"Found {len(imgfiles)} images and {len(labfiles)} labels in {folder}\")\n",
    "\n",
    "# Split the data into train, val, test\n",
    "random.shuffle(basenames)\n",
    "train = int(len(basenames) * PARAM['train'])\n",
    "val = int(len(basenames) * PARAM['val'])\n",
    "test = int(len(basenames) * PARAM['test'])\n",
    "print(f\"Split into {train} train, {val} val, {test} test. Total: {train+val+test} images.\")\n",
    "train_set = basenames[:train]\n",
    "val_set = basenames[train:train+val]\n",
    "test_set = basenames[train+val:]\n",
    "\n",
    "# Copy the images and labels to the new dataset folder\n",
    "for folder, dataset in zip(['train', 'val', 'test'], [train_set, val_set, test_set]):\n",
    "    for base in dataset:\n",
    "        filename = path.split(base)[1]\n",
    "        copy(path.join(PARAM['path'], f\"{base}.png\"), path.join(DATASET_FOLDER, 'images', folder, f\"{filename}.png\"))\n",
    "        base = base.replace('imgs', 'labels')\n",
    "        copy(path.join(PARAM['path'], f\"{base}.txt\"), path.join(DATASET_FOLDER, 'labels', folder, f\"{filename}.txt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6005 (pid 25652), started 13:30:46 ago. (Use '!kill 25652' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-e3e70682c2094cac\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-e3e70682c2094cac\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6005;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tensorboard logging\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir \"logs\" --port=6005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: The process \"40156\" not found.\n"
     ]
    }
   ],
   "source": [
    "! taskkill /PID 40156 /F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.2.18  Python-3.10.13 torch-2.1.1+cu121 CUDA:0 (NVIDIA GeForce RTX 3080 Laptop GPU, 16384MiB)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=obb, mode=train, model=./models/pretrained/yolov8n-obb-dotav1.pt, data=./models/recipes/dataset_desc.yaml, epochs=20, time=None, patience=10, batch=-1, imgsz=640, save=True, save_period=5, cache=False, device=0, workers=8, project=logs, name=all_run-20240521-124737, exist_ok=False, pretrained=True, optimizer=auto, verbose=False, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=True, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=False, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=4.0, cls=1.0, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.05, hsv_s=0.3, hsv_v=0.2, degrees=180, translate=0.1, scale=0.8, shear=10.0, perspective=0.0, flipud=0.5, fliplr=0.5, bgr=0.0, mosaic=0.5, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=logs\\all_run-20240521-124737\n",
      "Overriding model.yaml nc=15 with nc=11\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    825124  ultralytics.nn.modules.head.OBB              [11, 1, [64, 128, 256]]       \n",
      "YOLOv8n-obb summary: 250 layers, 3084660 parameters, 3084644 gradients, 8.5 GFLOPs\n",
      "\n",
      "Transferred 391/397 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir logs\\all_run-20240521-124737', view at http://localhost:6006/\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n",
      "\u001b[34m\u001b[1mAutoBatch: \u001b[0mComputing optimal batch size for imgsz=640\n",
      "\u001b[34m\u001b[1mAutoBatch: \u001b[0mCUDA:0 (NVIDIA GeForce RTX 3080 Laptop GPU) 16.00G total, 0.64G reserved, 0.07G allocated, 15.29G free\n",
      "      Params      GFLOPs  GPU_mem (GB)  forward (ms) backward (ms)                   input                  output\n",
      "     3084660       8.452         0.784         32.83           nan        (1, 3, 640, 640)                    list\n",
      "     3084660        16.9         0.623          47.5           nan        (2, 3, 640, 640)                    list\n",
      "     3084660       33.81         1.141            23           nan        (4, 3, 640, 640)                    list\n",
      "     3084660       67.62         1.921         22.67           nan        (8, 3, 640, 640)                    list\n",
      "     3084660       135.2         3.540         31.33           nan       (16, 3, 640, 640)                    list\n",
      "\u001b[34m\u001b[1mAutoBatch: \u001b[0mUsing batch-size 45 for CUDA:0 9.86G/16.00G (62%) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\Shaheen\\OneDrive - Imperial College London\\Uni\\CW Labs\\Year 4\\FYP\\src\\vision\\datasets\\current\\labels\\train.cache... 682 images, 0 backgrounds, 0 corrupt: 100%|██████████| 682/682 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Shaheen\\OneDrive - Imperial College London\\Uni\\CW Labs\\Year 4\\FYP\\src\\vision\\datasets\\current\\labels\\val.cache... 195 images, 0 backgrounds, 0 corrupt: 100%|██████████| 195/195 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000667, momentum=0.9) with parameter groups 63 weight(decay=0.0), 73 weight(decay=0.0003515625), 72 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added \n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mlogs\\all_run-20240521-124737\u001b[0m\n",
      "Starting training for 20 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/20      6.47G      1.057       11.1      2.472         13        640: 100%|██████████| 16/16 [00:06<00:00,  2.57it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:03<00:00,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        195      0.411      0.117      0.159     0.0911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/20      6.54G     0.7515      7.582       1.82         13        640: 100%|██████████| 16/16 [00:06<00:00,  2.62it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:02<00:00,  2.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        195      0.481      0.528      0.454      0.286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/20      6.55G     0.6713       4.87      1.614         10        640: 100%|██████████| 16/16 [00:05<00:00,  3.17it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  3.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        195      0.711      0.882       0.83      0.612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/20      6.55G     0.6369      3.497       1.57         10        640: 100%|██████████| 16/16 [00:04<00:00,  3.68it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  3.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        195      0.771      0.866      0.913      0.683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/20      6.54G     0.6105      2.807      1.558         11        640: 100%|██████████| 16/16 [00:04<00:00,  3.71it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  3.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        195      0.913      0.926      0.979      0.721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/20      6.54G     0.6057      2.385      1.535         11        640: 100%|██████████| 16/16 [00:04<00:00,  3.70it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  3.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        195      0.934      0.958      0.984      0.738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/20      6.54G     0.6077      2.157      1.584         12        640: 100%|██████████| 16/16 [00:04<00:00,  3.74it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  3.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        195      0.891      0.937      0.965       0.69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/20      6.54G     0.5979      1.862      1.561          7        640: 100%|██████████| 16/16 [00:04<00:00,  3.72it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  3.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        195      0.964      0.972      0.971      0.739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/20      6.54G     0.6126      1.799      1.585         12        640: 100%|██████████| 16/16 [00:04<00:00,  3.72it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  3.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        195      0.956      0.977      0.992      0.742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/20      6.54G     0.5852      1.668      1.535         17        640: 100%|██████████| 16/16 [00:04<00:00,  3.60it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  3.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        195      0.982      0.982      0.994      0.746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      11/20      6.54G     0.5418      1.461      1.568          7        640: 100%|██████████| 16/16 [00:04<00:00,  3.53it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  3.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        195       0.98      0.985      0.993      0.761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      12/20      6.54G     0.5294      1.381      1.542          7        640: 100%|██████████| 16/16 [00:04<00:00,  3.69it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  3.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        195       0.97      0.986      0.977      0.771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      13/20      6.54G     0.5281      1.349       1.55          7        640: 100%|██████████| 16/16 [00:04<00:00,  3.75it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  3.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        195      0.981          1      0.995      0.782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      14/20      6.54G     0.5192      1.233       1.54          7        640: 100%|██████████| 16/16 [00:04<00:00,  3.72it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  3.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        195      0.983       0.98      0.985      0.787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      15/20      6.54G     0.5207      1.184      1.553          7        640: 100%|██████████| 16/16 [00:04<00:00,  3.72it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  3.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        195      0.991          1      0.995      0.803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      16/20      6.54G     0.5176      1.198      1.577          7        640: 100%|██████████| 16/16 [00:04<00:00,  3.72it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  3.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        195      0.991          1      0.995      0.798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      17/20      6.54G     0.5131      1.169      1.525          7        640: 100%|██████████| 16/16 [00:04<00:00,  3.71it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  3.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        195      0.992          1      0.995      0.802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      18/20      6.54G     0.5011      1.116       1.52          7        640: 100%|██████████| 16/16 [00:04<00:00,  3.71it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  3.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        195      0.991          1      0.995      0.803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      19/20      6.54G     0.4976      1.081      1.493          7        640: 100%|██████████| 16/16 [00:04<00:00,  3.71it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  3.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        195      0.991          1      0.995      0.812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      20/20      6.54G     0.4975      1.079      1.467          7        640: 100%|██████████| 16/16 [00:04<00:00,  3.68it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  3.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        195      0.992          1      0.995      0.812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "20 epochs completed in 0.050 hours.\n",
      "Optimizer stripped from logs\\all_run-20240521-124737\\weights\\last.pt, 6.6MB\n",
      "Optimizer stripped from logs\\all_run-20240521-124737\\weights\\best.pt, 6.6MB\n",
      "\n",
      "Validating logs\\all_run-20240521-124737\\weights\\best.pt...\n",
      "Ultralytics YOLOv8.2.18  Python-3.10.13 torch-2.1.1+cu121 CUDA:0 (NVIDIA GeForce RTX 3080 Laptop GPU, 16384MiB)\n",
      "YOLOv8n-obb summary (fused): 187 layers, 3079364 parameters, 0 gradients, 8.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  3.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        195      0.991          1      0.995      0.812\n",
      "Speed: 0.3ms preprocess, 1.2ms inference, 0.0ms loss, 2.5ms postprocess per image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ultralytics.utils.metrics.OBBMetrics object with attributes:\n",
       "\n",
       "ap_class_index: array([ 0,  1,  2,  3,  7,  8, 10])\n",
       "box: ultralytics.utils.metrics.Metric object\n",
       "confusion_matrix: <ultralytics.utils.metrics.ConfusionMatrix object at 0x00000000157235B0>\n",
       "curves: []\n",
       "curves_results: []\n",
       "fitness: 0.8306406103475862\n",
       "keys: ['metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)']\n",
       "maps: array([    0.81603,     0.85872,     0.78105,     0.86127,     0.81238,     0.81238,     0.81238,     0.84235,     0.72344,     0.81238,      0.8038])\n",
       "names: {0: \"{0: 'resistors'}\", 1: \"{1: 'capacitors'}\", 2: \"{2: 'ceramic_cap'}\", 3: \"{3: 'inductors'}\", 4: \"{4: 'diodes'}\", 5: \"{5: 'mosfets'}\", 6: \"{6: 'transistors'}\", 7: \"{7: 'leds'}\", 8: \"{8: 'wires'}\", 9: \"{9: 'ics'}\", 10: \"{10: 'film_cap'}\"}\n",
       "plot: False\n",
       "results_dict: {'metrics/precision(B)': 0.991195177033917, 'metrics/recall(B)': 1.0, 'metrics/mAP50(B)': 0.995, 'metrics/mAP50-95(B)': 0.8123784559417624, 'fitness': 0.8306406103475862}\n",
       "save_dir: WindowsPath('logs/all_run-20240521-124737')\n",
       "speed: {'preprocess': 0.25635376954690003, 'inference': 1.1717833005464995, 'loss': 0.0, 'postprocess': 2.489775877732497}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train\n",
    "model = YOLO(MODEL_PATH).to('cuda')\n",
    "model.train(**TRAIN_PARAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "\nDataset 'models/recipes/dataset_desc.yaml' images not found ⚠️, missing path 'C:\\Users\\Shaheen\\OneDrive - Imperial College London\\Uni\\CW Labs\\Year 4\\FYP\\src\\vision\\datasets\\current\\images\\val'\nNote dataset download directory is 'C:\\Users\\Shaheen\\OneDrive - Imperial College London\\Uni\\CW Labs\\Year 4\\FYP\\src\\vision\\datasets'. You can update this in 'C:\\Users\\Shaheen\\AppData\\Roaming\\Ultralytics\\settings.yaml'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Test the model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTRAIN_PARAM\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Shaheen\\Anaconda3\\envs\\fyp\\lib\\site-packages\\ultralytics\\engine\\model.py:528\u001b[0m, in \u001b[0;36mModel.val\u001b[1;34m(self, validator, **kwargs)\u001b[0m\n\u001b[0;32m    525\u001b[0m args \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moverrides, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcustom, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m}  \u001b[38;5;66;03m# highest priority args on the right\u001b[39;00m\n\u001b[0;32m    527\u001b[0m validator \u001b[38;5;241m=\u001b[39m (validator \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_smart_load(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidator\u001b[39m\u001b[38;5;124m\"\u001b[39m))(args\u001b[38;5;241m=\u001b[39margs, _callbacks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks)\n\u001b[1;32m--> 528\u001b[0m \u001b[43mvalidator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    529\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics \u001b[38;5;241m=\u001b[39m validator\u001b[38;5;241m.\u001b[39mmetrics\n\u001b[0;32m    530\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m validator\u001b[38;5;241m.\u001b[39mmetrics\n",
      "File \u001b[1;32mc:\\Users\\Shaheen\\Anaconda3\\envs\\fyp\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Shaheen\\Anaconda3\\envs\\fyp\\lib\\site-packages\\ultralytics\\engine\\validator.py:143\u001b[0m, in \u001b[0;36mBaseValidator.__call__\u001b[1;34m(self, trainer, model)\u001b[0m\n\u001b[0;32m    140\u001b[0m     LOGGER\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mForcing batch=1 square inference (1,3,\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimgsz\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimgsz\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) for non-PyTorch models\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdata)\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myaml\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myml\u001b[39m\u001b[38;5;124m\"\u001b[39m}:\n\u001b[1;32m--> 143\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_det_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtask \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassify\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m check_cls_dataset(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdata, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msplit)\n",
      "File \u001b[1;32mc:\\Users\\Shaheen\\Anaconda3\\envs\\fyp\\lib\\site-packages\\ultralytics\\data\\utils.py:329\u001b[0m, in \u001b[0;36mcheck_det_dataset\u001b[1;34m(dataset, autodownload)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    328\u001b[0m     m \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mNote dataset download directory is \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDATASETS_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. You can update this in \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSETTINGS_YAML\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 329\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(m)\n\u001b[0;32m    330\u001b[0m t \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    331\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# success\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: \nDataset 'models/recipes/dataset_desc.yaml' images not found ⚠️, missing path 'C:\\Users\\Shaheen\\OneDrive - Imperial College London\\Uni\\CW Labs\\Year 4\\FYP\\src\\vision\\datasets\\current\\images\\val'\nNote dataset download directory is 'C:\\Users\\Shaheen\\OneDrive - Imperial College London\\Uni\\CW Labs\\Year 4\\FYP\\src\\vision\\datasets'. You can update this in 'C:\\Users\\Shaheen\\AppData\\Roaming\\Ultralytics\\settings.yaml'"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "metrics = model.val(data=TRAIN_PARAM['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image_to_bbox(image, bbox):\n",
    "    \"\"\"\n",
    "    Crop the image to the region within the bounding box.\n",
    "\n",
    "    Args:\n",
    "    - image (np.array): The input image.\n",
    "    - bbox (tensor): The coordinates of the bounding box in xyxyxyxy format.\n",
    "\n",
    "    Returns:\n",
    "    - cropped_image (np.array): The cropped image.\n",
    "    \"\"\"\n",
    "    # Convert tensor to numpy array and ensure it's on the CPU\n",
    "    bbox = bbox.cpu().numpy()[0]\n",
    "\n",
    "    # Compute the width and height of the bounding box\n",
    "    width = int(np.linalg.norm(bbox[0] - bbox[1]))\n",
    "    height = int(np.linalg.norm(bbox[1] - bbox[2]))\n",
    "\n",
    "    # Compute the center of the bounding box\n",
    "    center = np.mean(bbox, axis=0).astype(int)\n",
    "\n",
    "    # Compute the rotation angle of the bounding box\n",
    "    angle = np.degrees(np.arctan2(bbox[1, 1] - bbox[0, 1], bbox[1, 0] - bbox[0, 0]))\n",
    "    rotation_matrix = cv2.getRotationMatrix2D(tuple(center), angle, 1.0)\n",
    "\n",
    "    # Apply the rotation to the image\n",
    "    rotated_image = cv2.warpAffine(image, rotation_matrix, (image.shape[1], image.shape[0]))\n",
    "\n",
    "    # Get the bounding box in the rotated image\n",
    "    x, y = center - [width // 2, height // 2]\n",
    "    cropped_image = rotated_image[y:y+height, x:x+width]\n",
    "\n",
    "    return cropped_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grab component in box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image_to_bbox(image, bbox):\n",
    "    \"\"\"\n",
    "    Crop the image to the region within the bounding box.\n",
    "\n",
    "    Args:\n",
    "    - image (np.array): The input image.\n",
    "    - bbox (tensor): The coordinates of the bounding box in xyxyxyxy format.\n",
    "\n",
    "    Returns:\n",
    "    - cropped_image (np.array): The cropped image.\n",
    "    \"\"\"\n",
    "    # Convert tensor to numpy array and ensure it's on the CPU\n",
    "    bbox = bbox.cpu().numpy()[0]\n",
    "\n",
    "    # Compute the width and height of the bounding box\n",
    "    width = int(np.linalg.norm(bbox[0] - bbox[1]))\n",
    "    height = int(np.linalg.norm(bbox[1] - bbox[2]))\n",
    "\n",
    "    # Compute the center of the bounding box\n",
    "    center = np.mean(bbox, axis=0).astype(int)\n",
    "    print(center)\n",
    "\n",
    "    # Compute the rotation angle of the bounding box\n",
    "    angle = np.degrees(np.arctan2(bbox[1, 1] - bbox[0, 1], bbox[1, 0] - bbox[0, 0]))\n",
    "    rotation_matrix = cv2.getRotationMatrix2D((int(center[0]), int(center[1])), angle, 1.0)\n",
    "\n",
    "    # Apply the rotation to the image\n",
    "    rotated_image = cv2.warpAffine(image, rotation_matrix, (image.shape[1], image.shape[0]))\n",
    "\n",
    "    # Get the bounding box in the rotated image\n",
    "    x, y = center - [width // 2, height // 2]\n",
    "    cropped_image = rotated_image[y:y+height, x:x+width]\n",
    "\n",
    "    return cropped_image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded trained model\n",
      "Random file: obb_capacitor_330uF-16V_6.png\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 1/1 c:\\Users\\Shaheen\\OneDrive - Imperial College London\\Uni\\CW Labs\\Year 4\\FYP\\src\\vision\\datasets\\full\\current\\images\\test\\obb_capacitor_330uF-16V_6.png: 480x640 138.5ms\n",
      "Speed: 371.0ms preprocess, 138.5ms inference, 59.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[263 202]\n",
      "Random file: obb_ceramic_capacitor_121_11.png\n",
      "\n",
      "image 1/1 c:\\Users\\Shaheen\\OneDrive - Imperial College London\\Uni\\CW Labs\\Year 4\\FYP\\src\\vision\\datasets\\full\\current\\images\\test\\obb_ceramic_capacitor_121_11.png: 480x640 151.0ms\n",
      "Speed: 3.0ms preprocess, 151.0ms inference, 16.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[568 217]\n",
      "Random file: obb_inductor_104_7.png\n",
      "\n",
      "image 1/1 c:\\Users\\Shaheen\\OneDrive - Imperial College London\\Uni\\CW Labs\\Year 4\\FYP\\src\\vision\\datasets\\full\\current\\images\\test\\obb_inductor_104_7.png: 480x640 121.5ms\n",
      "Speed: 3.0ms preprocess, 121.5ms inference, 24.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[401 344]\n",
      "Random file: obb_resistor_yellow_violet_black_gold_brown_470E-1-1_16.png\n",
      "\n",
      "image 1/1 c:\\Users\\Shaheen\\OneDrive - Imperial College London\\Uni\\CW Labs\\Year 4\\FYP\\src\\vision\\datasets\\full\\current\\images\\test\\obb_resistor_yellow_violet_black_gold_brown_470E-1-1_16.png: 480x640 18.0ms\n",
      "Speed: 3.0ms preprocess, 18.0ms inference, 9.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[577 175]\n",
      "Random file: obb_capacitor_1uF-50V_5.png\n",
      "\n",
      "image 1/1 c:\\Users\\Shaheen\\OneDrive - Imperial College London\\Uni\\CW Labs\\Year 4\\FYP\\src\\vision\\datasets\\full\\current\\images\\test\\obb_capacitor_1uF-50V_5.png: 480x640 42.5ms\n",
      "Speed: 2.5ms preprocess, 42.5ms inference, 8.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[294 374]\n",
      "Random file: obb_resistor_yellow_violet_black_gold_brown_470E-1-1_16.png\n",
      "\n",
      "image 1/1 c:\\Users\\Shaheen\\OneDrive - Imperial College London\\Uni\\CW Labs\\Year 4\\FYP\\src\\vision\\datasets\\full\\current\\images\\test\\obb_resistor_yellow_violet_black_gold_brown_470E-1-1_16.png: 480x640 35.5ms\n",
      "Speed: 2.5ms preprocess, 35.5ms inference, 4.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[577 175]\n",
      "Random file: obb_resistor_yellow_orange_black_silver_brown_brown_430E-2-1_5.png\n",
      "\n",
      "image 1/1 c:\\Users\\Shaheen\\OneDrive - Imperial College London\\Uni\\CW Labs\\Year 4\\FYP\\src\\vision\\datasets\\full\\current\\images\\test\\obb_resistor_yellow_orange_black_silver_brown_brown_430E-2-1_5.png: 480x640 42.0ms\n",
      "Speed: 2.5ms preprocess, 42.0ms inference, 5.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[275 150]\n",
      "Random file: obb_ceramic_capacitor_10_6.png\n",
      "\n",
      "image 1/1 c:\\Users\\Shaheen\\OneDrive - Imperial College London\\Uni\\CW Labs\\Year 4\\FYP\\src\\vision\\datasets\\full\\current\\images\\test\\obb_ceramic_capacitor_10_6.png: 480x640 47.0ms\n",
      "Speed: 2.5ms preprocess, 47.0ms inference, 16.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[362 116]\n",
      "Random file: obb_ceramic_capacitor_391_6.png\n",
      "\n",
      "image 1/1 c:\\Users\\Shaheen\\OneDrive - Imperial College London\\Uni\\CW Labs\\Year 4\\FYP\\src\\vision\\datasets\\full\\current\\images\\test\\obb_ceramic_capacitor_391_6.png: 480x640 52.5ms\n",
      "Speed: 4.5ms preprocess, 52.5ms inference, 13.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[372  63]\n",
      "Random file: obb_resistor_brown_black_black_brown_brown_100E1-1_6.png\n",
      "\n",
      "image 1/1 c:\\Users\\Shaheen\\OneDrive - Imperial College London\\Uni\\CW Labs\\Year 4\\FYP\\src\\vision\\datasets\\full\\current\\images\\test\\obb_resistor_brown_black_black_brown_brown_100E1-1_6.png: 480x640 16.5ms\n",
      "Speed: 2.5ms preprocess, 16.5ms inference, 3.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[124  75]\n"
     ]
    }
   ],
   "source": [
    "DATA = PATHS['test']\n",
    "\n",
    "try:\n",
    "    model\n",
    "    print(\"Loaded trained model\")\n",
    "except:\n",
    "    # Load the best model\n",
    "    # List dir and find latest run\n",
    "    runs = listdir(f\"./logs\")\n",
    "    runs.sort()\n",
    "    while True:\n",
    "        latest_run = runs.pop()\n",
    "        if path.isdir(f\"./logs/{latest_run}\") and RECIPE_NAME in latest_run:\n",
    "            break\n",
    "    modelPath = f\"./logs/{latest_run}/weights/best.pt\"\n",
    "    print(f\"Latest run: {latest_run}, loading model from {modelPath}\")\n",
    "    bestModel = YOLO(modelPath).to('cuda')\n",
    "    model = bestModel\n",
    "    print(\"Loaded best model\")\n",
    "\n",
    "while cv2.waitKey(0) != ord('q'):\n",
    "    cv2.destroyAllWindows()\n",
    "    randomFile = random.choice(listdir(DATA))\n",
    "    print(f\"Random file: {randomFile}\")\n",
    "    results = model.predict(f\"{DATA}/{randomFile}\", show=True)\n",
    "    cropped_img = crop_image_to_bbox(results[0].orig_img, results[0].obb.xyxyxyxy)\n",
    "    cv2.imshow(\"Cropped Image\", cropped_img)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw vertices to verify direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random file: obb_resistor_brown_brown_black_orange_brown_red_110E3-1_8.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shaheen\\AppData\\Local\\Temp\\ipykernel_37104\\1325440353.py:39: DeprecationWarning: `np.int0` is a deprecated alias for `np.intp`.  (Deprecated NumPy 1.24)\n",
      "  box = np.array(bbox[0].cpu(), dtype=np.int0)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def draw_bounding_box(image, bbox, bbox_format, color=(0, 255, 0), thickness=2):\n",
    "    \"\"\"\n",
    "    Draws a bounding box on the image with an arrow indicating orientation for the xyxyxyxy format.\n",
    "\n",
    "    Args:\n",
    "    - image (np.array): The input image.\n",
    "    - bbox (tensor): The bounding box coordinates.\n",
    "    - bbox_format (str): The format of the bounding box ('xywhr', 'xyxy', 'xyxyxyxy', 'xyxyxyxyn').\n",
    "    - color (tuple): The color of the bounding box (default is green).\n",
    "    - thickness (int): The thickness of the bounding box lines (default is 2).\n",
    "\n",
    "    Returns:\n",
    "    - image_with_box (np.array): The image with the bounding box drawn.\n",
    "    \"\"\"\n",
    "    image_with_box = image.copy()  # Make a copy of the image to avoid overwriting\n",
    "\n",
    "    if bbox_format == 'xywhr':\n",
    "        # xywhr: [x_center, y_center, width, height, rotation (in radians)]\n",
    "        x_center, y_center, width, height, rotation = bbox[0]\n",
    "        center = (int(x_center.item()), int(y_center.item()))\n",
    "        size = (int(width.item()), int(height.item()))\n",
    "        angle = np.degrees(rotation.item())\n",
    "\n",
    "        rect = ((center[0], center[1]), (size[0], size[1]), angle)\n",
    "        box = cv2.boxPoints(rect)\n",
    "        box = np.int0(box)\n",
    "\n",
    "    elif bbox_format == 'xyxy':\n",
    "        # xyxy: [x_min, y_min, x_max, y_max]\n",
    "        x_min, y_min, x_max, y_max = bbox[0]\n",
    "        box = np.array([[x_min.item(), y_min.item()],\n",
    "                        [x_max.item(), y_min.item()],\n",
    "                        [x_max.item(), y_max.item()],\n",
    "                        [x_min.item(), y_max.item()]], dtype=np.int0)\n",
    "\n",
    "    elif bbox_format == 'xyxyxyxy':\n",
    "        # xyxyxyxy: [[x1, y1], [x2, y2], [x3, y3], [x4, y4]]\n",
    "        box = np.array(bbox[0].cpu(), dtype=np.int0)\n",
    "\n",
    "    elif bbox_format == 'xyxyxyxyn':\n",
    "        # xyxyxyxyn: normalized [[x1, y1], [x2, y2], [x3, y3], [x4, y4]]\n",
    "        h, w = image.shape[:2]\n",
    "        box = np.array(bbox[0].cpu() * np.array([w, h]), dtype=np.int0)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported bbox_format: {bbox_format}\")\n",
    "\n",
    "    # Draw the polygon\n",
    "    image_with_box = cv2.polylines(image_with_box, [box], isClosed=True, color=color, thickness=thickness)\n",
    "\n",
    "    # Draw an arrow for the xyxyxyxy format to indicate orientation\n",
    "    if bbox_format == 'xyxyxyxy':\n",
    "        start_point = (int(box[0][0]), int(box[0][1]))\n",
    "        end_point = (int(box[1][0]), int(box[1][1]))\n",
    "        image_with_box = cv2.arrowedLine(image_with_box, start_point, end_point, color, thickness, tipLength=0.3)\n",
    "\n",
    "     # Number each vertex\n",
    "    for i, (x, y) in enumerate(box):\n",
    "        cv2.putText(image_with_box, str(i+1), (x, y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), thickness, cv2.LINE_AA)\n",
    "\n",
    "    return image_with_box\n",
    "\n",
    "DATA = PATHS['resistors']\n",
    "randomFile = random.choice(listdir(DATA))\n",
    "print(f\"Random file: {randomFile}\")\n",
    "results = model.predict(f\"{DATA}/{randomFile}\", show=True)\n",
    "\n",
    "cv2.imshow(randomFile, draw_bounding_box(results[0].orig_img, results[0].obb.xyxyxyxy, 'xyxyxyxy'))\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[198.1221,  44.0472],\n",
      "         [217.5405, 135.7320],\n",
      "         [343.3661, 109.0828],\n",
      "         [323.9478,  17.3980]]], device='cuda:0')\n",
      "[[[198.1221466064453, 44.0472412109375], [217.54051208496094, 135.7319793701172], [343.3661193847656, 109.082763671875], [323.9477844238281, 17.398029327392578]]]\n",
      "[198.1221466064453, 44.0472412109375]\n"
     ]
    }
   ],
   "source": [
    "for result in results:\n",
    "    print(result.obb.xyxyxyxy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "from os import path, listdir, makedirs\n",
    "from shutil import rmtree, copy\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the full classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "IMAGE_SIZE = 640 # x 480\n",
    "PROJECT_NAME = \"logs\"\n",
    "RECIPE_NAME = \"all\"\n",
    "TRAIN_RUN = f\"{RECIPE_NAME}_run-{time.strftime('%Y%m%d-%H%M%S')}\"\n",
    "DATA_PATH = \"./datasets/full\"\n",
    "\n",
    "# Model\n",
    "MODEL_PATH = \"./models/pretrained/yolov8n-obb-dotav1.pt\"\n",
    "TEST_PATH = \"./models/recipes/dataset_desc_test.yaml\"\n",
    "TRAIN_PARAM = {\n",
    "    # Model definition\n",
    "    'data': \"./models/recipes/dataset_desc.yaml\",\n",
    "    'resume': False,\n",
    "    'device': '0',\n",
    "    'pretrained': True,\n",
    "    # Names\n",
    "    'project' : PROJECT_NAME,\n",
    "    'name': TRAIN_RUN,\n",
    "    # Training Parameters\n",
    "    'batch': -1,\n",
    "    'imgsz': IMAGE_SIZE,\n",
    "    'epochs': 50,\n",
    "    'patience': 5,\n",
    "    'cos_lr': True,\n",
    "    # \"lr0\": 0.05,\n",
    "    # Augmentation\n",
    "    'hsv_h': 0.05, # Higher than default for resistor\n",
    "    'hsv_s': 0.3, # Colours should not change too much\n",
    "    'hsv_v': 0.2, # Colours should not change too much\n",
    "    'degrees': 180, # Rotation\n",
    "    'translate': 0.1, # Translation\n",
    "    'scale': 0.8, # Scaling - camera is always at the same distance\n",
    "    'shear': 10.0, # Shearing\n",
    "    'perspective': 0.0, # Perspective\n",
    "    'flipud': 0.5, # Flip up-down\n",
    "    'fliplr': 0.5, # Flip left-right\n",
    "    'mosaic': 0.5, # Mosaic\n",
    "    'mixup': 0.0, # Mixup\n",
    "    'copy_paste': 0.0, # Copy-paste\n",
    "    'crop_fraction': 1.0, # Crop fraction\n",
    "    # Loss weights\n",
    "    # 'cls' : 1.0, # Class\n",
    "    # 'box' : 4.0, # Box accuracy\n",
    "    # 'dfl' : 1.5, # Help manage unbalanced classes\n",
    "    # Post parameters\n",
    "    'save': True,\n",
    "    'save_period': 5,\n",
    "    'plots': False,\n",
    "    # Misc\n",
    "    'verbose': False,\n",
    "}\n",
    "\n",
    "PATHS = {\n",
    "    'train': f\"{DATA_PATH}/current/images/train\",\n",
    "    'val': f\"{DATA_PATH}/current/images/val\",\n",
    "    'test': f\"{DATA_PATH}/current/images/test\",\n",
    "    'resistors' : f\"{DATA_PATH}/resistor/imgs\",\n",
    "    'ceramic_cap' : f\"{DATA_PATH}/ceramic_capacitor/imgs\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 164 images and 164 labels in capacitor\n",
      "Found 264 images and 264 labels in ceramic_capacitor\n",
      "Found 66 images and 66 labels in film_capacitor\n",
      "Found 52 images and 52 labels in inductor\n",
      "Found 96 images and 96 labels in led\n",
      "Found 258 images and 259 labels in resistor\n",
      "Found 75 images and 75 labels in wire\n",
      "Split into 682 train, 195 val, 97 test. Total: 974 images.\n"
     ]
    }
   ],
   "source": [
    "# Reorganise the data\n",
    "PARAM = {\n",
    "    'path' : DATA_PATH,\n",
    "    'train' : 0.7,\n",
    "    'val' : 0.2,\n",
    "    'test' : 0.1,\n",
    "    'include' : [\n",
    "        'resistor',\n",
    "        'capacitor',\n",
    "        'ceramic_capacitor',\n",
    "        'film_capacitor',\n",
    "        'inductor',\n",
    "        'led',\n",
    "        'wire'\n",
    "    ]\n",
    "}\n",
    "# Remove old dataset\n",
    "DATASET_FOLDER = f\"{DATA_PATH}/current\"\n",
    "if path.exists(DATASET_FOLDER):\n",
    "    for folder in listdir(DATASET_FOLDER):\n",
    "        rmtree(path.join(DATASET_FOLDER, folder), ignore_errors=True)\n",
    "    # Make label folder\n",
    "    makedirs(path.join(DATASET_FOLDER, 'labels'), exist_ok=True)\n",
    "    # Make train, val, test folders\n",
    "    for folder in ['train', 'val', 'test']:\n",
    "        makedirs(path.join(DATASET_FOLDER, 'images', folder), exist_ok=True)\n",
    "        makedirs(path.join(DATASET_FOLDER, 'labels', folder), exist_ok=True)\n",
    "\n",
    "# Get all component images from all folders\n",
    "basenames = []\n",
    "# For component folder in dataset\n",
    "for folder in listdir(PARAM['path']):\n",
    "    # Skip if not in include\n",
    "    if folder not in PARAM['include']: continue\n",
    "    # For each subfolder in component folder\n",
    "    imgfiles = listdir(path.join(PARAM['path'], folder, 'imgs'))\n",
    "    labfiles = listdir(path.join(PARAM['path'], folder, 'labels'))\n",
    "    bases = [path.join(folder, 'imgs', path.splitext(f)[0]) for f in imgfiles]\n",
    "    basenames.extend(bases)\n",
    "    print(f\"Found {len(imgfiles)} images and {len(labfiles)} labels in {folder}\")\n",
    "\n",
    "# Split the data into train, val, test\n",
    "random.shuffle(basenames)\n",
    "train = int(len(basenames) * PARAM['train'])\n",
    "val = int(len(basenames) * PARAM['val'])\n",
    "test = int(len(basenames) * PARAM['test'])\n",
    "print(f\"Split into {train} train, {val} val, {test} test. Total: {train+val+test} images.\")\n",
    "train_set = basenames[:train]\n",
    "val_set = basenames[train:train+val]\n",
    "test_set = basenames[train+val:]\n",
    "\n",
    "# Copy the images and labels to the new dataset folder\n",
    "for folder, dataset in zip(['train', 'val', 'test'], [train_set, val_set, test_set]):\n",
    "    for base in dataset:\n",
    "        filename = path.split(base)[1]\n",
    "        copy(path.join(PARAM['path'], f\"{base}.png\"), path.join(DATASET_FOLDER, 'images', folder, f\"{filename}.png\"))\n",
    "        base = base.replace('imgs', 'labels')\n",
    "        copy(path.join(PARAM['path'], f\"{base}.txt\"), path.join(DATASET_FOLDER, 'labels', folder, f\"{filename}.txt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6004 (pid 8464), started 0:03:38 ago. (Use '!kill 8464' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-55465fe55eab8f24\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-55465fe55eab8f24\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6004;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tensorboard logging\n",
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir \"logs\" --port=6004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: The process \"25652\" not found.\n",
      "ERROR: The process \"tensorboard.exe\" not found.\n"
     ]
    }
   ],
   "source": [
    "! taskkill /PID 25652 /F\n",
    "! taskkill /IM \"tensorboard.exe\" /F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.2.30 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.2.20  Python-3.10.13 torch-2.1.1+cu121 CUDA:0 (NVIDIA GeForce RTX 3080 Laptop GPU, 16384MiB)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=obb, mode=train, model=./models/pretrained/yolov8n-obb-dotav1.pt, data=./models/recipes/dataset_desc.yaml, epochs=50, time=None, patience=5, batch=-1, imgsz=640, save=True, save_period=5, cache=False, device=0, workers=8, project=logs, name=all_run-20240610-045510, exist_ok=False, pretrained=True, optimizer=auto, verbose=False, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=True, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=False, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.05, hsv_s=0.3, hsv_v=0.2, degrees=180, translate=0.1, scale=0.8, shear=10.0, perspective=0.0, flipud=0.5, fliplr=0.5, bgr=0.0, mosaic=0.5, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=logs\\all_run-20240610-045510\n",
      "Overriding model.yaml nc=15 with nc=11\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    825124  ultralytics.nn.modules.head.OBB              [11, 1, [64, 128, 256]]       \n",
      "YOLOv8n-obb summary: 250 layers, 3084660 parameters, 3084644 gradients, 8.5 GFLOPs\n",
      "\n",
      "Transferred 391/397 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir logs\\all_run-20240610-045510', view at http://localhost:6006/\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n",
      "\u001b[34m\u001b[1mAutoBatch: \u001b[0mComputing optimal batch size for imgsz=640\n",
      "\u001b[34m\u001b[1mAutoBatch: \u001b[0mCUDA:0 (NVIDIA GeForce RTX 3080 Laptop GPU) 16.00G total, 0.34G reserved, 0.08G allocated, 15.58G free\n",
      "      Params      GFLOPs  GPU_mem (GB)  forward (ms) backward (ms)                   input                  output\n",
      "     3084660       8.452         0.503            35           nan        (1, 3, 640, 640)                    list\n",
      "     3084660        16.9         0.633         28.33           nan        (2, 3, 640, 640)                    list\n",
      "     3084660       33.81         1.170         36.83           nan        (4, 3, 640, 640)                    list\n",
      "     3084660       67.62         2.020         32.17           nan        (8, 3, 640, 640)                    list\n",
      "     3084660       135.2         3.540         48.17           nan       (16, 3, 640, 640)                    list\n",
      "\u001b[34m\u001b[1mAutoBatch: \u001b[0mUsing batch-size 44 for CUDA:0 9.74G/16.00G (61%) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\Shaheen\\OneDrive - Imperial College London\\Uni\\CW Labs\\Year 4\\FYP\\src\\vision\\datasets\\full\\current\\labels\\train.cache... 682 images, 0 backgrounds, 0 corrupt: 100%|██████████| 682/682 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Shaheen\\OneDrive - Imperial College London\\Uni\\CW Labs\\Year 4\\FYP\\src\\vision\\datasets\\full\\current\\labels\\val.cache... 195 images, 0 backgrounds, 0 corrupt: 100%|██████████| 195/195 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000667, momentum=0.9) with parameter groups 63 weight(decay=0.0), 73 weight(decay=0.00034375), 72 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added \n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mlogs\\all_run-20240610-045510\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/50       6.5G      1.958      5.587      2.462         37        640: 100%|██████████| 16/16 [00:06<00:00,  2.40it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:03<00:00,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        195      0.548     0.0981      0.159     0.0894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/50      6.42G       1.36      3.806      1.797         33        640: 100%|██████████| 16/16 [00:04<00:00,  3.45it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:02<00:00,  2.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        195      0.272      0.456      0.361      0.231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/50      6.43G      1.154      2.407       1.53         46        640: 100%|██████████| 16/16 [00:04<00:00,  3.39it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  3.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        195      0.641      0.844      0.772      0.571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/50      6.43G      1.128      1.828      1.555         31        640: 100%|██████████| 16/16 [00:04<00:00,  3.48it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  3.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        195      0.747      0.884      0.813      0.606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/50      6.42G      1.123      1.382      1.512         41        640: 100%|██████████| 16/16 [00:04<00:00,  3.48it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  3.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        195      0.741      0.869      0.871      0.666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/50      6.42G      1.072      1.168      1.525         34        640: 100%|██████████| 16/16 [00:04<00:00,  3.54it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  3.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        195      0.913      0.882       0.92      0.702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/50      6.42G      1.138      1.022      1.578         32        640: 100%|██████████| 16/16 [00:04<00:00,  3.53it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  3.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        195      0.901      0.941      0.951      0.748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/50      6.42G      1.077     0.8768      1.558         43        640: 100%|██████████| 16/16 [00:04<00:00,  3.48it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  3.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        195      0.952      0.947      0.975      0.749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/50      6.42G       1.11     0.9071      1.609         36        640: 100%|██████████| 16/16 [00:04<00:00,  3.48it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  3.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        195      0.975      0.928      0.969      0.738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/50      6.42G      1.071     0.7891      1.564         36        640: 100%|██████████| 16/16 [00:04<00:00,  3.46it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  3.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        195      0.924      0.942      0.957      0.727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      11/50      6.42G      1.066     0.7825      1.615         30        640: 100%|██████████| 16/16 [00:04<00:00,  3.47it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  3.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        195      0.956      0.957      0.993      0.763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      12/50      6.42G      1.044      0.734      1.576         29        640: 100%|██████████| 16/16 [00:04<00:00,  3.44it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        195      0.951      0.987      0.976      0.752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      13/50      6.42G      1.022     0.7006      1.557         37        640: 100%|██████████| 16/16 [00:04<00:00,  3.41it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  3.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        195      0.989      0.979      0.995      0.764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      14/50      6.42G      1.064     0.7141      1.567         29        640: 100%|██████████| 16/16 [00:04<00:00,  3.41it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  3.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        195       0.99      0.998      0.995      0.794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      15/50      6.42G      1.049     0.6854      1.573         41        640: 100%|██████████| 16/16 [00:04<00:00,  3.46it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  3.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        195      0.965          1      0.992      0.783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      16/50      6.42G      1.016     0.6479       1.54         37        640: 100%|██████████| 16/16 [00:04<00:00,  3.26it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  3.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        195      0.966      0.973      0.977      0.748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      17/50      6.42G      1.041     0.6464      1.589         35        640: 100%|██████████| 16/16 [00:04<00:00,  3.44it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  3.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        195      0.982      0.982      0.994      0.775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      18/50      6.42G      1.041     0.6528       1.58         33        640: 100%|██████████| 16/16 [00:04<00:00,  3.36it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  3.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        195       0.98      0.998      0.995      0.799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      19/50      6.42G     0.9865     0.6017      1.528         41        640: 100%|██████████| 16/16 [00:04<00:00,  3.49it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  3.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        195      0.975          1      0.995       0.79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      20/50      6.42G      1.008     0.6204      1.574         33        640: 100%|██████████| 16/16 [00:04<00:00,  3.49it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  3.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        195       0.99          1      0.995      0.775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      21/50      6.42G      1.001     0.5937      1.539         35        640: 100%|██████████| 16/16 [00:04<00:00,  3.44it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  3.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        195      0.992      0.999      0.995       0.81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      22/50      6.42G      1.018     0.5986      1.593         24        640: 100%|██████████| 16/16 [00:04<00:00,  3.42it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  3.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        195      0.982      0.981      0.992      0.778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      23/50      6.42G      1.014     0.5897      1.549         43        640: 100%|██████████| 16/16 [00:04<00:00,  3.44it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  3.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        195      0.979          1      0.995      0.792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      24/50      6.42G     0.9777     0.5756      1.539         28        640: 100%|██████████| 16/16 [00:04<00:00,  3.38it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  2.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        195      0.985      0.994      0.995      0.794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      25/50      6.42G     0.9761      0.574       1.53         31        640: 100%|██████████| 16/16 [00:05<00:00,  3.10it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  3.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        195      0.986      0.998      0.995      0.802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      26/50      6.42G     0.9683     0.5679      1.555         32        640: 100%|██████████| 16/16 [00:04<00:00,  3.21it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  3.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        195      0.985      0.997      0.995      0.803\n",
      "\u001b[34m\u001b[1mEarlyStopping: \u001b[0mTraining stopped early as no improvement observed in last 5 epochs. Best results observed at epoch 21, best model saved as best.pt.\n",
      "To update EarlyStopping(patience=5) pass a new patience value, i.e. `patience=300` or use `patience=0` to disable EarlyStopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "26 epochs completed in 0.053 hours.\n",
      "Optimizer stripped from logs\\all_run-20240610-045510\\weights\\last.pt, 6.7MB\n",
      "Optimizer stripped from logs\\all_run-20240610-045510\\weights\\best.pt, 6.7MB\n",
      "\n",
      "Validating logs\\all_run-20240610-045510\\weights\\best.pt...\n",
      "Ultralytics YOLOv8.2.20  Python-3.10.13 torch-2.1.1+cu121 CUDA:0 (NVIDIA GeForce RTX 3080 Laptop GPU, 16384MiB)\n",
      "YOLOv8n-obb summary (fused): 187 layers, 3079364 parameters, 0 gradients, 8.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  3.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        195      0.992      0.998      0.995      0.811\n",
      "Speed: 0.2ms preprocess, 1.1ms inference, 0.0ms loss, 2.7ms postprocess per image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ultralytics.utils.metrics.OBBMetrics object with attributes:\n",
       "\n",
       "ap_class_index: array([ 0,  1,  2,  3,  7,  8, 10])\n",
       "box: ultralytics.utils.metrics.Metric object\n",
       "confusion_matrix: <ultralytics.utils.metrics.ConfusionMatrix object at 0x0000000054FC1300>\n",
       "curves: []\n",
       "curves_results: []\n",
       "fitness: 0.8293837684566568\n",
       "keys: ['metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)']\n",
       "maps: array([    0.81038,     0.85989,     0.79021,     0.88478,     0.81098,     0.81098,     0.81098,     0.88259,     0.61691,     0.81098,     0.83211])\n",
       "names: {0: \"{0: 'resistor'}\", 1: \"{1: 'capacitor'}\", 2: \"{2: 'ceramic_cap'}\", 3: \"{3: 'inductors'}\", 4: \"{4: 'diodes'}\", 5: \"{5: 'mosfet'}\", 6: \"{6: 'transistor'}\", 7: \"{7: 'leds'}\", 8: \"{8: 'wire'}\", 9: \"{9: 'ics'}\", 10: \"{10: 'film_cap'}\"}\n",
       "plot: False\n",
       "results_dict: {'metrics/precision(B)': 0.9918011066627745, 'metrics/recall(B)': 0.9984425442758776, 'metrics/mAP50(B)': 0.995, 'metrics/mAP50-95(B)': 0.8109819649518408, 'fitness': 0.8293837684566568}\n",
       "save_dir: WindowsPath('logs/all_run-20240610-045510')\n",
       "speed: {'preprocess': 0.2462105873303536, 'inference': 1.0820193168444512, 'loss': 0.0, 'postprocess': 2.6743546510354066}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train\n",
    "model = YOLO(MODEL_PATH).to('cuda')\n",
    "model.train(**TRAIN_PARAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.2.20  Python-3.10.13 torch-2.1.1+cu121 CUDA:0 (NVIDIA GeForce RTX 3080 Laptop GPU, 16384MiB)\n",
      "YOLOv8n-obb summary (fused): 187 layers, 3079364 parameters, 0 gradients, 8.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Shaheen\\OneDrive - Imperial College London\\Uni\\CW Labs\\Year 4\\FYP\\src\\vision\\datasets\\full\\current\\labels\\test.cache... 98 images, 0 backgrounds, 0 corrupt: 100%|██████████| 98/98 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:09<00:00,  3.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         98         98      0.985      0.994      0.995      0.806\n",
      "Speed: 3.6ms preprocess, 13.4ms inference, 0.0ms loss, 4.9ms postprocess per image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "metrics = model.val(data=TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image_to_bbox(image, bbox):\n",
    "    \"\"\"\n",
    "    Crop the image to the region within the bounding box.\n",
    "\n",
    "    Args:\n",
    "    - image (np.array): The input image.\n",
    "    - bbox (tensor): The coordinates of the bounding box in xyxyxyxy format.\n",
    "\n",
    "    Returns:\n",
    "    - cropped_image (np.array): The cropped image.\n",
    "    \"\"\"\n",
    "    # Convert tensor to numpy array and ensure it's on the CPU\n",
    "    bbox = bbox.cpu().numpy()[0]\n",
    "\n",
    "    # Compute the width and height of the bounding box\n",
    "    width = int(np.linalg.norm(bbox[0] - bbox[1]))\n",
    "    height = int(np.linalg.norm(bbox[1] - bbox[2]))\n",
    "\n",
    "    # Compute the center of the bounding box\n",
    "    center = np.mean(bbox, axis=0).astype(int)\n",
    "\n",
    "    # Compute the rotation angle of the bounding box\n",
    "    angle = np.degrees(np.arctan2(bbox[1, 1] - bbox[0, 1], bbox[1, 0] - bbox[0, 0]))\n",
    "    rotation_matrix = cv2.getRotationMatrix2D(tuple(center), angle, 1.0)\n",
    "\n",
    "    # Apply the rotation to the image\n",
    "    rotated_image = cv2.warpAffine(image, rotation_matrix, (image.shape[1], image.shape[0]))\n",
    "\n",
    "    # Get the bounding box in the rotated image\n",
    "    x, y = center - [width // 2, height // 2]\n",
    "    cropped_image = rotated_image[y:y+height, x:x+width]\n",
    "\n",
    "    return cropped_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grab component in box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image_to_bbox(image, bbox):\n",
    "    \"\"\"\n",
    "    Crop the image to the region within the bounding box.\n",
    "\n",
    "    Args:\n",
    "    - image (np.array): The input image.\n",
    "    - bbox (tensor): The coordinates of the bounding box in xyxyxyxy format.\n",
    "\n",
    "    Returns:\n",
    "    - cropped_image (np.array): The cropped image.\n",
    "    \"\"\"\n",
    "    # Convert tensor to numpy array and ensure it's on the CPU\n",
    "    bbox = bbox.cpu().numpy()[0]\n",
    "\n",
    "    # Compute the width and height of the bounding box\n",
    "    width = int(np.linalg.norm(bbox[0] - bbox[1]))\n",
    "    height = int(np.linalg.norm(bbox[1] - bbox[2]))\n",
    "\n",
    "    # Compute the center of the bounding box\n",
    "    center = np.mean(bbox, axis=0).astype(int)\n",
    "    print(center)\n",
    "\n",
    "    # Compute the rotation angle of the bounding box\n",
    "    angle = np.degrees(np.arctan2(bbox[1, 1] - bbox[0, 1], bbox[1, 0] - bbox[0, 0]))\n",
    "    rotation_matrix = cv2.getRotationMatrix2D((int(center[0]), int(center[1])), angle, 1.0)\n",
    "\n",
    "    # Apply the rotation to the image\n",
    "    rotated_image = cv2.warpAffine(image, rotation_matrix, (image.shape[1], image.shape[0]))\n",
    "\n",
    "    # Get the bounding box in the rotated image\n",
    "    x, y = center - [width // 2, height // 2]\n",
    "    cropped_image = rotated_image[y:y+height, x:x+width]\n",
    "\n",
    "    return cropped_image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded trained model\n",
      "Random file: obb_capacitor_330uF-16V_6.png\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 1/1 c:\\Users\\Shaheen\\OneDrive - Imperial College London\\Uni\\CW Labs\\Year 4\\FYP\\src\\vision\\datasets\\full\\current\\images\\test\\obb_capacitor_330uF-16V_6.png: 480x640 138.5ms\n",
      "Speed: 371.0ms preprocess, 138.5ms inference, 59.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[263 202]\n",
      "Random file: obb_ceramic_capacitor_121_11.png\n",
      "\n",
      "image 1/1 c:\\Users\\Shaheen\\OneDrive - Imperial College London\\Uni\\CW Labs\\Year 4\\FYP\\src\\vision\\datasets\\full\\current\\images\\test\\obb_ceramic_capacitor_121_11.png: 480x640 151.0ms\n",
      "Speed: 3.0ms preprocess, 151.0ms inference, 16.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[568 217]\n",
      "Random file: obb_inductor_104_7.png\n",
      "\n",
      "image 1/1 c:\\Users\\Shaheen\\OneDrive - Imperial College London\\Uni\\CW Labs\\Year 4\\FYP\\src\\vision\\datasets\\full\\current\\images\\test\\obb_inductor_104_7.png: 480x640 121.5ms\n",
      "Speed: 3.0ms preprocess, 121.5ms inference, 24.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[401 344]\n",
      "Random file: obb_resistor_yellow_violet_black_gold_brown_470E-1-1_16.png\n",
      "\n",
      "image 1/1 c:\\Users\\Shaheen\\OneDrive - Imperial College London\\Uni\\CW Labs\\Year 4\\FYP\\src\\vision\\datasets\\full\\current\\images\\test\\obb_resistor_yellow_violet_black_gold_brown_470E-1-1_16.png: 480x640 18.0ms\n",
      "Speed: 3.0ms preprocess, 18.0ms inference, 9.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[577 175]\n",
      "Random file: obb_capacitor_1uF-50V_5.png\n",
      "\n",
      "image 1/1 c:\\Users\\Shaheen\\OneDrive - Imperial College London\\Uni\\CW Labs\\Year 4\\FYP\\src\\vision\\datasets\\full\\current\\images\\test\\obb_capacitor_1uF-50V_5.png: 480x640 42.5ms\n",
      "Speed: 2.5ms preprocess, 42.5ms inference, 8.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[294 374]\n",
      "Random file: obb_resistor_yellow_violet_black_gold_brown_470E-1-1_16.png\n",
      "\n",
      "image 1/1 c:\\Users\\Shaheen\\OneDrive - Imperial College London\\Uni\\CW Labs\\Year 4\\FYP\\src\\vision\\datasets\\full\\current\\images\\test\\obb_resistor_yellow_violet_black_gold_brown_470E-1-1_16.png: 480x640 35.5ms\n",
      "Speed: 2.5ms preprocess, 35.5ms inference, 4.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[577 175]\n",
      "Random file: obb_resistor_yellow_orange_black_silver_brown_brown_430E-2-1_5.png\n",
      "\n",
      "image 1/1 c:\\Users\\Shaheen\\OneDrive - Imperial College London\\Uni\\CW Labs\\Year 4\\FYP\\src\\vision\\datasets\\full\\current\\images\\test\\obb_resistor_yellow_orange_black_silver_brown_brown_430E-2-1_5.png: 480x640 42.0ms\n",
      "Speed: 2.5ms preprocess, 42.0ms inference, 5.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[275 150]\n",
      "Random file: obb_ceramic_capacitor_10_6.png\n",
      "\n",
      "image 1/1 c:\\Users\\Shaheen\\OneDrive - Imperial College London\\Uni\\CW Labs\\Year 4\\FYP\\src\\vision\\datasets\\full\\current\\images\\test\\obb_ceramic_capacitor_10_6.png: 480x640 47.0ms\n",
      "Speed: 2.5ms preprocess, 47.0ms inference, 16.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[362 116]\n",
      "Random file: obb_ceramic_capacitor_391_6.png\n",
      "\n",
      "image 1/1 c:\\Users\\Shaheen\\OneDrive - Imperial College London\\Uni\\CW Labs\\Year 4\\FYP\\src\\vision\\datasets\\full\\current\\images\\test\\obb_ceramic_capacitor_391_6.png: 480x640 52.5ms\n",
      "Speed: 4.5ms preprocess, 52.5ms inference, 13.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[372  63]\n",
      "Random file: obb_resistor_brown_black_black_brown_brown_100E1-1_6.png\n",
      "\n",
      "image 1/1 c:\\Users\\Shaheen\\OneDrive - Imperial College London\\Uni\\CW Labs\\Year 4\\FYP\\src\\vision\\datasets\\full\\current\\images\\test\\obb_resistor_brown_black_black_brown_brown_100E1-1_6.png: 480x640 16.5ms\n",
      "Speed: 2.5ms preprocess, 16.5ms inference, 3.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[124  75]\n"
     ]
    }
   ],
   "source": [
    "DATA = PATHS['test']\n",
    "\n",
    "try:\n",
    "    model\n",
    "    print(\"Loaded trained model\")\n",
    "except:\n",
    "    # Load the best model\n",
    "    # List dir and find latest run\n",
    "    runs = listdir(f\"./logs\")\n",
    "    runs.sort()\n",
    "    while True:\n",
    "        latest_run = runs.pop()\n",
    "        if path.isdir(f\"./logs/{latest_run}\") and RECIPE_NAME in latest_run:\n",
    "            break\n",
    "    modelPath = f\"./logs/{latest_run}/weights/best.pt\"\n",
    "    print(f\"Latest run: {latest_run}, loading model from {modelPath}\")\n",
    "    bestModel = YOLO(modelPath).to('cuda')\n",
    "    model = bestModel\n",
    "    print(\"Loaded best model\")\n",
    "\n",
    "while cv2.waitKey(0) != ord('q'):\n",
    "    cv2.destroyAllWindows()\n",
    "    randomFile = random.choice(listdir(DATA))\n",
    "    print(f\"Random file: {randomFile}\")\n",
    "    results = model.predict(f\"{DATA}/{randomFile}\", show=True)\n",
    "    cropped_img = crop_image_to_bbox(results[0].orig_img, results[0].obb.xyxyxyxy)\n",
    "    cv2.imshow(\"Cropped Image\", cropped_img)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw vertices to verify direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random file: obb_resistor_brown_brown_black_orange_brown_red_110E3-1_8.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shaheen\\AppData\\Local\\Temp\\ipykernel_37104\\1325440353.py:39: DeprecationWarning: `np.int0` is a deprecated alias for `np.intp`.  (Deprecated NumPy 1.24)\n",
      "  box = np.array(bbox[0].cpu(), dtype=np.int0)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def draw_bounding_box(image, bbox, bbox_format, color=(0, 255, 0), thickness=2):\n",
    "    \"\"\"\n",
    "    Draws a bounding box on the image with an arrow indicating orientation for the xyxyxyxy format.\n",
    "\n",
    "    Args:\n",
    "    - image (np.array): The input image.\n",
    "    - bbox (tensor): The bounding box coordinates.\n",
    "    - bbox_format (str): The format of the bounding box ('xywhr', 'xyxy', 'xyxyxyxy', 'xyxyxyxyn').\n",
    "    - color (tuple): The color of the bounding box (default is green).\n",
    "    - thickness (int): The thickness of the bounding box lines (default is 2).\n",
    "\n",
    "    Returns:\n",
    "    - image_with_box (np.array): The image with the bounding box drawn.\n",
    "    \"\"\"\n",
    "    image_with_box = image.copy()  # Make a copy of the image to avoid overwriting\n",
    "\n",
    "    if bbox_format == 'xywhr':\n",
    "        # xywhr: [x_center, y_center, width, height, rotation (in radians)]\n",
    "        x_center, y_center, width, height, rotation = bbox[0]\n",
    "        center = (int(x_center.item()), int(y_center.item()))\n",
    "        size = (int(width.item()), int(height.item()))\n",
    "        angle = np.degrees(rotation.item())\n",
    "\n",
    "        rect = ((center[0], center[1]), (size[0], size[1]), angle)\n",
    "        box = cv2.boxPoints(rect)\n",
    "        box = np.int0(box)\n",
    "\n",
    "    elif bbox_format == 'xyxy':\n",
    "        # xyxy: [x_min, y_min, x_max, y_max]\n",
    "        x_min, y_min, x_max, y_max = bbox[0]\n",
    "        box = np.array([[x_min.item(), y_min.item()],\n",
    "                        [x_max.item(), y_min.item()],\n",
    "                        [x_max.item(), y_max.item()],\n",
    "                        [x_min.item(), y_max.item()]], dtype=np.int0)\n",
    "\n",
    "    elif bbox_format == 'xyxyxyxy':\n",
    "        # xyxyxyxy: [[x1, y1], [x2, y2], [x3, y3], [x4, y4]]\n",
    "        box = np.array(bbox[0].cpu(), dtype=np.int0)\n",
    "\n",
    "    elif bbox_format == 'xyxyxyxyn':\n",
    "        # xyxyxyxyn: normalized [[x1, y1], [x2, y2], [x3, y3], [x4, y4]]\n",
    "        h, w = image.shape[:2]\n",
    "        box = np.array(bbox[0].cpu() * np.array([w, h]), dtype=np.int0)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported bbox_format: {bbox_format}\")\n",
    "\n",
    "    # Draw the polygon\n",
    "    image_with_box = cv2.polylines(image_with_box, [box], isClosed=True, color=color, thickness=thickness)\n",
    "\n",
    "    # Draw an arrow for the xyxyxyxy format to indicate orientation\n",
    "    if bbox_format == 'xyxyxyxy':\n",
    "        start_point = (int(box[0][0]), int(box[0][1]))\n",
    "        end_point = (int(box[1][0]), int(box[1][1]))\n",
    "        image_with_box = cv2.arrowedLine(image_with_box, start_point, end_point, color, thickness, tipLength=0.3)\n",
    "\n",
    "     # Number each vertex\n",
    "    for i, (x, y) in enumerate(box):\n",
    "        cv2.putText(image_with_box, str(i+1), (x, y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), thickness, cv2.LINE_AA)\n",
    "\n",
    "    return image_with_box\n",
    "\n",
    "DATA = PATHS['resistors']\n",
    "randomFile = random.choice(listdir(DATA))\n",
    "print(f\"Random file: {randomFile}\")\n",
    "results = model.predict(f\"{DATA}/{randomFile}\", show=True)\n",
    "\n",
    "cv2.imshow(randomFile, draw_bounding_box(results[0].orig_img, results[0].obb.xyxyxyxy, 'xyxyxyxy'))\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[198.1221,  44.0472],\n",
      "         [217.5405, 135.7320],\n",
      "         [343.3661, 109.0828],\n",
      "         [323.9478,  17.3980]]], device='cuda:0')\n",
      "[[[198.1221466064453, 44.0472412109375], [217.54051208496094, 135.7319793701172], [343.3661193847656, 109.082763671875], [323.9477844238281, 17.398029327392578]]]\n",
      "[198.1221466064453, 44.0472412109375]\n"
     ]
    }
   ],
   "source": [
    "for result in results:\n",
    "    print(result.obb.xyxyxyxy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "from os import path, listdir, makedirs\n",
    "from shutil import rmtree, copy\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the full classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "IMAGE_SIZE = 640 # x 480\n",
    "PROJECT_NAME = \"logs\"\n",
    "RECIPE_NAME = \"all\"\n",
    "TRAIN_RUN = f\"{RECIPE_NAME}_run-{time.strftime('%Y%m%d-%H%M%S')}\"\n",
    "DATA_PATH = \"./../../datasets/full\"\n",
    "\n",
    "# Model\n",
    "MODEL_PATH = \"./models/pretrained/yolov8n-obb-dotav1.pt\"\n",
    "TEST_PATH = \"./models/recipes/dataset_desc_test.yaml\"\n",
    "VAL_PATH = \"./models/recipes/dataset_desc.yaml\"\n",
    "TRAIN_PARAM = {\n",
    "    # Model definition\n",
    "    'data': VAL_PATH,\n",
    "    'resume': False,\n",
    "    'device': '0',\n",
    "    'pretrained': True,\n",
    "    # Names\n",
    "    'project' : PROJECT_NAME,\n",
    "    'name': TRAIN_RUN,\n",
    "    # Training Parameters\n",
    "    'batch': -1,\n",
    "    'imgsz': IMAGE_SIZE,\n",
    "    'epochs': 50,\n",
    "    'patience': 5,\n",
    "    'cos_lr': True,\n",
    "    # \"lr0\": 0.05,\n",
    "    # Augmentation\n",
    "    'hsv_h': 0.05, # Higher than default for resistor\n",
    "    'hsv_s': 0.3, # Colours should not change too much\n",
    "    'hsv_v': 0.2, # Colours should not change too much\n",
    "    'degrees': 180, # Rotation\n",
    "    'translate': 0.1, # Translation\n",
    "    'scale': 0.8, # Scaling - camera is always at the same distance\n",
    "    'shear': 10.0, # Shearing\n",
    "    'perspective': 0.0, # Perspective\n",
    "    'flipud': 0.5, # Flip up-down\n",
    "    'fliplr': 0.5, # Flip left-right\n",
    "    'mosaic': 0.5, # Mosaic\n",
    "    'mixup': 0.0, # Mixup\n",
    "    'copy_paste': 0.0, # Copy-paste\n",
    "    'crop_fraction': 1.0, # Crop fraction\n",
    "    # Loss weights\n",
    "    # 'cls' : 1.0, # Class\n",
    "    # 'box' : 4.0, # Box accuracy\n",
    "    # 'dfl' : 1.5, # Help manage unbalanced classes\n",
    "    # Post parameters\n",
    "    'save': True,\n",
    "    'save_period': 5,\n",
    "    'plots': False,\n",
    "    # Misc\n",
    "    'verbose': False,\n",
    "}\n",
    "\n",
    "PATHS = {\n",
    "    'train': f\"{DATA_PATH}/current/images/train\",\n",
    "    'val': f\"{DATA_PATH}/current/images/val\",\n",
    "    'test': f\"{DATA_PATH}/current/images/test\",\n",
    "    'resistors' : f\"{DATA_PATH}/resistor/imgs\",\n",
    "    'ceramic_cap' : f\"{DATA_PATH}/ceramic_capacitor/imgs\",\n",
    "    \"none\" : \"./datasets/none\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 164 images and 164 labels in capacitor\n",
      "Found 264 images and 264 labels in ceramic_capacitor\n",
      "Found 66 images and 66 labels in film_capacitor\n",
      "Found 52 images and 52 labels in inductor\n",
      "Found 96 images and 96 labels in led\n",
      "Found 258 images and 259 labels in resistor\n",
      "Found 75 images and 75 labels in wire\n",
      "Split into 682 train, 195 val, 97 test. Total: 974 images.\n"
     ]
    }
   ],
   "source": [
    "# Reorganise the data\n",
    "PARAM = {\n",
    "    'path' : DATA_PATH,\n",
    "    'train' : 0.7,\n",
    "    'val' : 0.2,\n",
    "    'test' : 0.1,\n",
    "    'include' : [\n",
    "        'resistor',\n",
    "        'capacitor',\n",
    "        'ceramic_capacitor',\n",
    "        'film_capacitor',\n",
    "        'inductor',\n",
    "        'led',\n",
    "        'wire'\n",
    "    ]\n",
    "}\n",
    "# Remove old dataset\n",
    "DATASET_FOLDER = f\"{DATA_PATH}/current\"\n",
    "if path.exists(DATASET_FOLDER):\n",
    "    for folder in listdir(DATASET_FOLDER):\n",
    "        rmtree(path.join(DATASET_FOLDER, folder), ignore_errors=True)\n",
    "    # Make label folder\n",
    "    makedirs(path.join(DATASET_FOLDER, 'labels'), exist_ok=True)\n",
    "    # Make train, val, test folders\n",
    "    for folder in ['train', 'val', 'test']:\n",
    "        makedirs(path.join(DATASET_FOLDER, 'images', folder), exist_ok=True)\n",
    "        makedirs(path.join(DATASET_FOLDER, 'labels', folder), exist_ok=True)\n",
    "\n",
    "# Get all component images from all folders\n",
    "basenames = []\n",
    "# For component folder in dataset\n",
    "for folder in listdir(PARAM['path']):\n",
    "    # Skip if not in include\n",
    "    if folder not in PARAM['include']: continue\n",
    "    # For each subfolder in component folder\n",
    "    imgfiles = listdir(path.join(PARAM['path'], folder, 'imgs'))\n",
    "    labfiles = listdir(path.join(PARAM['path'], folder, 'labels'))\n",
    "    bases = [path.join(folder, 'imgs', path.splitext(f)[0]) for f in imgfiles]\n",
    "    basenames.extend(bases)\n",
    "    print(f\"Found {len(imgfiles)} images and {len(labfiles)} labels in {folder}\")\n",
    "\n",
    "# Split the data into train, val, test\n",
    "random.shuffle(basenames)\n",
    "train = int(len(basenames) * PARAM['train'])\n",
    "val = int(len(basenames) * PARAM['val'])\n",
    "test = int(len(basenames) * PARAM['test'])\n",
    "print(f\"Split into {train} train, {val} val, {test} test. Total: {train+val+test} images.\")\n",
    "train_set = basenames[:train]\n",
    "val_set = basenames[train:train+val]\n",
    "test_set = basenames[train+val:]\n",
    "\n",
    "# Copy the images and labels to the new dataset folder\n",
    "for folder, dataset in zip(['train', 'val', 'test'], [train_set, val_set, test_set]):\n",
    "    for base in dataset:\n",
    "        filename = path.split(base)[1]\n",
    "        copy(path.join(PARAM['path'], f\"{base}.png\"), path.join(DATASET_FOLDER, 'images', folder, f\"{filename}.png\"))\n",
    "        base = base.replace('imgs', 'labels')\n",
    "        copy(path.join(PARAM['path'], f\"{base}.txt\"), path.join(DATASET_FOLDER, 'labels', folder, f\"{filename}.txt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-8c778ea6eb2083e6\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-8c778ea6eb2083e6\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6005;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tensorboard logging\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir \"logs\" --port=6005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: The process \"9424\" not found.\n",
      "ERROR: The process \"tensorboard.exe\" not found.\n"
     ]
    }
   ],
   "source": [
    "! taskkill /PID 9424 /F\n",
    "! taskkill /IM \"tensorboard.exe\" /F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.2.32 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.2.31  Python-3.10.13 torch-2.1.1+cu121 CUDA:0 (NVIDIA GeForce RTX 3080 Laptop GPU, 16384MiB)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=obb, mode=train, model=./models/pretrained/yolov8n-obb-dotav1.pt, data=./models/recipes/dataset_desc.yaml, epochs=50, time=None, patience=5, batch=-1, imgsz=640, save=True, save_period=5, cache=False, device=0, workers=8, project=logs, name=all_run-20240614-0444198, exist_ok=False, pretrained=True, optimizer=auto, verbose=False, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=True, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=False, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.05, hsv_s=0.3, hsv_v=0.2, degrees=180, translate=0.1, scale=0.8, shear=10.0, perspective=0.0, flipud=0.5, fliplr=0.5, bgr=0.0, mosaic=0.5, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=logs\\all_run-20240614-0444198\n",
      "Overriding model.yaml nc=15 with nc=11\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    825124  ultralytics.nn.modules.head.OBB              [11, 1, [64, 128, 256]]       \n",
      "YOLOv8n-obb summary: 250 layers, 3084660 parameters, 3084644 gradients, 8.5 GFLOPs\n",
      "\n",
      "Transferred 391/397 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir logs\\all_run-20240614-0444198', view at http://localhost:6006/\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n",
      "\u001b[34m\u001b[1mAutoBatch: \u001b[0mComputing optimal batch size for imgsz=640 at 60.0% CUDA memory utilization.\n",
      "\u001b[34m\u001b[1mAutoBatch: \u001b[0mCUDA:0 (NVIDIA GeForce RTX 3080 Laptop GPU) 16.00G total, 1.35G reserved, 0.17G allocated, 14.49G free\n",
      "      Params      GFLOPs  GPU_mem (GB)  forward (ms) backward (ms)                   input                  output\n",
      "     3084660       8.452         1.581            72           nan        (1, 3, 640, 640)                    list\n",
      "     3084660        16.9         0.786         22.17           nan        (2, 3, 640, 640)                    list\n",
      "     3084660       33.81         1.191          22.5           nan        (4, 3, 640, 640)                    list\n",
      "     3084660       67.62         2.112            26           nan        (8, 3, 640, 640)                    list\n",
      "     3084660       135.2         3.641          32.5           nan       (16, 3, 640, 640)                    list\n",
      "\u001b[34m\u001b[1mAutoBatch: \u001b[0mUsing batch-size 46 for CUDA:0 10.15G/16.00G (63%) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\Shaheen\\OneDrive - Imperial College London\\Uni\\CW Labs\\Year 4\\FYP\\datasets\\full\\current\\labels\\train... 682 images, 72 backgrounds, 0 corrupt: 100%|██████████| 754/754 [00:00<00:00, 1300.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: C:\\Users\\Shaheen\\OneDrive - Imperial College London\\Uni\\CW Labs\\Year 4\\FYP\\datasets\\full\\current\\labels\\train.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Shaheen\\OneDrive - Imperial College London\\Uni\\CW Labs\\Year 4\\FYP\\datasets\\full\\current\\labels\\val.cache... 195 images, 20 backgrounds, 0 corrupt: 100%|██████████| 215/215 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000667, momentum=0.9) with parameter groups 63 weight(decay=0.0), 73 weight(decay=0.000359375), 72 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added \n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mlogs\\all_run-20240614-0444198\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/50      6.62G      1.942      5.735      2.465         18        640: 100%|██████████| 17/17 [00:13<00:00,  1.23it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:04<00:00,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        215        195      0.421      0.148       0.17      0.112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/50      6.53G      1.345      3.862      1.738         23        640: 100%|██████████| 17/17 [00:05<00:00,  3.24it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:02<00:00,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        215        195      0.371      0.596      0.508      0.349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/50      6.53G       1.18      2.326      1.563         20        640: 100%|██████████| 17/17 [00:06<00:00,  2.79it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  2.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        215        195      0.503      0.727      0.654      0.483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/50      6.52G      1.134      1.805      1.516         25        640: 100%|██████████| 17/17 [00:06<00:00,  2.79it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:04<00:00,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        215        195      0.807      0.853      0.899       0.68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/50      6.52G      1.085       1.45      1.505         52        640:  35%|███▌      | 6/17 [00:02<00:03,  3.00it/s]"
     ]
    }
   ],
   "source": [
    "# Raw model\n",
    "# model = YOLO(\"yolov8n-obb.yaml\").to('cuda')\n",
    "\n",
    "# Train\n",
    "model = YOLO(MODEL_PATH).to('cuda')\n",
    "model.train(**TRAIN_PARAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.2.31  Python-3.10.13 torch-2.1.1+cu121 CUDA:0 (NVIDIA GeForce RTX 3080 Laptop GPU, 16384MiB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Shaheen\\OneDrive - Imperial College London\\Uni\\CW Labs\\Year 4\\FYP\\datasets\\full\\current\\labels\\val... 195 images, 20 backgrounds, 0 corrupt: 100%|██████████| 215/215 [00:00<00:00, 1067.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: C:\\Users\\Shaheen\\OneDrive - Imperial College London\\Uni\\CW Labs\\Year 4\\FYP\\datasets\\full\\current\\labels\\val.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:02<00:00,  1.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        215        195      0.833      0.948      0.984      0.759\n",
      "Speed: 1.2ms preprocess, 2.9ms inference, 0.0ms loss, 2.4ms postprocess per image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "metrics = model.val(data=VAL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[          0           0           0           0           0           0           0           0           0           0           0           0]\n",
      " [          0           0           0           0           0           0           0           0           0           0           0           0]\n",
      " [          0           0           0           0           0           0           0           0           0           0           0           0]\n",
      " [          0           0           0           0           0           0           0           0           0           0           0           0]\n",
      " [          0           0           0           0           0           0           0           0           0           0           0           0]\n",
      " [          0           0           0           0           0           0           0           0           0           0           0           0]\n",
      " [          0           0           0           0           0           0           0           0           0           0           0           0]\n",
      " [          0           0           0           0           0           0           0           0           0           0           0           0]\n",
      " [          0           0           0           0           0           0           0           0           0           0           0           0]\n",
      " [          0           0           0           0           0           0           0           0           0           0           0           0]\n",
      " [          0           0           0           0           0           0           0           0           0           0           0           0]\n",
      " [          0           0           0           0           0           0           0           0           0           0           0           0]]\n",
      "[[          0           0           0           0           0           0           0           0]\n",
      " [          0           0           0           0           0           0           0           0]\n",
      " [          0           0           0           0           0           0           0           0]\n",
      " [          0           0           0           0           0           0           0           0]\n",
      " [          0           0           0           0           0           0           0           0]\n",
      " [          0           0           0           0           0           0           0           0]\n",
      " [          0           0           0           0           0           0           0           0]\n",
      " [          0           0           0           0           0           0           0           0]]\n"
     ]
    }
   ],
   "source": [
    "confusion_matrix = metrics.confusion_matrix.matrix\n",
    "\n",
    "remove = [4, 5, 6, 9]\n",
    "filtered_matrix = np.delete(confusion_matrix, remove, axis=0)\n",
    "filtered_matrix = np.delete(filtered_matrix, remove, axis=1)\n",
    "print(confusion_matrix)\n",
    "print(filtered_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.confusion_matrix.matrix = filtered_matrix\n",
    "metrics.confusion_matrix.nc = 7\n",
    "metrics.confusion_matrix.plot(names=('resistor', 'capacitor', 'ceramic_capacitor', \"inductor\", \"led\", \"wire\", \"film_capacitor\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: \"{0: 'resistor'}\",\n",
       " 1: \"{1: 'capacitor'}\",\n",
       " 2: \"{2: 'ceramic_cap'}\",\n",
       " 3: \"{3: 'inductors'}\",\n",
       " 4: \"{4: 'diodes'}\",\n",
       " 5: \"{5: 'mosfet'}\",\n",
       " 6: \"{6: 'transistor'}\",\n",
       " 7: \"{7: 'leds'}\",\n",
       " 8: \"{8: 'wire'}\",\n",
       " 9: \"{9: 'ics'}\",\n",
       " 10: \"{10: 'film_cap'}\"}"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grab component in box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image_to_bbox(image, bbox):\n",
    "    \"\"\"\n",
    "    Crop the image to the region within the bounding box.\n",
    "\n",
    "    Args:\n",
    "    - image (np.array): The input image.\n",
    "    - bbox (tensor): The coordinates of the bounding box in xyxyxyxy format.\n",
    "\n",
    "    Returns:\n",
    "    - cropped_image (np.array): The cropped image.\n",
    "    \"\"\"\n",
    "    # Convert tensor to numpy array and ensure it's on the CPU\n",
    "    bbox = bbox.cpu().numpy()[0]\n",
    "\n",
    "    # Compute the width and height of the bounding box\n",
    "    width = int(np.linalg.norm(bbox[0] - bbox[1]))\n",
    "    height = int(np.linalg.norm(bbox[1] - bbox[2]))\n",
    "\n",
    "    # Compute the center of the bounding box\n",
    "    center = np.mean(bbox, axis=0).astype(int)\n",
    "\n",
    "    # Compute the rotation angle of the bounding box\n",
    "    angle = np.degrees(np.arctan2(bbox[1, 1] - bbox[0, 1], bbox[1, 0] - bbox[0, 0]))\n",
    "    rotation_matrix = cv2.getRotationMatrix2D((int(center[0]), int(center[1])), angle, 1.0)\n",
    "\n",
    "    # Apply the rotation to the image\n",
    "    rotated_image = cv2.warpAffine(image, rotation_matrix, (image.shape[1], image.shape[0]))\n",
    "\n",
    "    # Get the bounding box in the rotated image\n",
    "    x, y = center - [width // 2, height // 2]\n",
    "    cropped_image = rotated_image[y:y+height, x:x+width]\n",
    "\n",
    "    return cropped_image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest run: all_run-20240614-041748, loading model from ./logs/all_run-20240614-041748/weights/best.pt\n",
      "Loaded best model\n",
      "Random file: obb_inductor_473_19.png\n",
      "\n",
      "image 1/1 c:\\Users\\Shaheen\\OneDrive - Imperial College London\\Uni\\CW Labs\\Year 4\\FYP\\src\\vision\\..\\..\\datasets\\full\\current\\images\\test\\obb_inductor_473_19.png: 480x640 11.0ms\n",
      "Speed: 1.5ms preprocess, 11.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[284  67]\n",
      "Random file: obb_ceramic_capacitor_102_14.png\n",
      "\n",
      "image 1/1 c:\\Users\\Shaheen\\OneDrive - Imperial College London\\Uni\\CW Labs\\Year 4\\FYP\\src\\vision\\..\\..\\datasets\\full\\current\\images\\test\\obb_ceramic_capacitor_102_14.png: 480x640 15.5ms\n",
      "Speed: 182.5ms preprocess, 15.5ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[357  78]\n",
      "Random file: obb_led_red_2.png\n",
      "\n",
      "image 1/1 c:\\Users\\Shaheen\\OneDrive - Imperial College London\\Uni\\CW Labs\\Year 4\\FYP\\src\\vision\\..\\..\\datasets\\full\\current\\images\\test\\obb_led_red_2.png: 480x640 11.5ms\n",
      "Speed: 1.0ms preprocess, 11.5ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[300 177]\n",
      "Random file: obb_film_capacitor_104j_8.png\n",
      "\n",
      "image 1/1 c:\\Users\\Shaheen\\OneDrive - Imperial College London\\Uni\\CW Labs\\Year 4\\FYP\\src\\vision\\..\\..\\datasets\\full\\current\\images\\test\\obb_film_capacitor_104j_8.png: 480x640 10.5ms\n",
      "Speed: 3.0ms preprocess, 10.5ms inference, 3.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[398 242]\n",
      "Random file: obb_led_green_1.png\n",
      "\n",
      "image 1/1 c:\\Users\\Shaheen\\OneDrive - Imperial College London\\Uni\\CW Labs\\Year 4\\FYP\\src\\vision\\..\\..\\datasets\\full\\current\\images\\test\\obb_led_green_1.png: 480x640 16.5ms\n",
      "Speed: 2.0ms preprocess, 16.5ms inference, 6.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[137 111]\n",
      "Random file: obb_ceramic_capacitor_391_3.png\n",
      "\n",
      "image 1/1 c:\\Users\\Shaheen\\OneDrive - Imperial College London\\Uni\\CW Labs\\Year 4\\FYP\\src\\vision\\..\\..\\datasets\\full\\current\\images\\test\\obb_ceramic_capacitor_391_3.png: 480x640 16.5ms\n",
      "Speed: 2.0ms preprocess, 16.5ms inference, 3.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[468 221]\n",
      "Random file: 2023-10-22_23-52-37_blender.jpg\n",
      "\n",
      "image 1/1 c:\\Users\\Shaheen\\OneDrive - Imperial College London\\Uni\\CW Labs\\Year 4\\FYP\\src\\vision\\..\\..\\datasets\\full\\current\\images\\test\\2023-10-22_23-52-37_blender.jpg: 512x640 (no detections), 224.5ms\n",
      "Speed: 3.0ms preprocess, 224.5ms inference, 1.4ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Random file: 2023-11-07_15-54-52_PotPlayerMini64.jpg\n",
      "\n",
      "image 1/1 c:\\Users\\Shaheen\\OneDrive - Imperial College London\\Uni\\CW Labs\\Year 4\\FYP\\src\\vision\\..\\..\\datasets\\full\\current\\images\\test\\2023-11-07_15-54-52_PotPlayerMini64.jpg: 416x640 (no detections), 210.5ms\n",
      "Speed: 3.5ms preprocess, 210.5ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Random file: obb_inductor_473_27.png\n",
      "\n",
      "image 1/1 c:\\Users\\Shaheen\\OneDrive - Imperial College London\\Uni\\CW Labs\\Year 4\\FYP\\src\\vision\\..\\..\\datasets\\full\\current\\images\\test\\obb_inductor_473_27.png: 480x640 12.0ms\n",
      "Speed: 2.0ms preprocess, 12.0ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[335 267]\n",
      "Random file: obb_resistor_orange_white_black_brown_brown_390E1-1_14.png\n",
      "\n",
      "image 1/1 c:\\Users\\Shaheen\\OneDrive - Imperial College London\\Uni\\CW Labs\\Year 4\\FYP\\src\\vision\\..\\..\\datasets\\full\\current\\images\\test\\obb_resistor_orange_white_black_brown_brown_390E1-1_14.png: 480x640 10.5ms\n",
      "Speed: 2.5ms preprocess, 10.5ms inference, 3.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[436 108]\n",
      "Random file: obb_resistor_yellow_violet_black_gold_brown_470E-1-1_28.png\n",
      "\n",
      "image 1/1 c:\\Users\\Shaheen\\OneDrive - Imperial College London\\Uni\\CW Labs\\Year 4\\FYP\\src\\vision\\..\\..\\datasets\\full\\current\\images\\test\\obb_resistor_yellow_violet_black_gold_brown_470E-1-1_28.png: 480x640 149.0ms\n",
      "Speed: 3.0ms preprocess, 149.0ms inference, 3.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[361 190]\n"
     ]
    }
   ],
   "source": [
    "DATA = PATHS['test']\n",
    "\n",
    "try:\n",
    "    model\n",
    "    print(\"Loaded trained model\")\n",
    "except:\n",
    "    # Load the best model\n",
    "    # List dir and find latest run\n",
    "    runs = listdir(f\"./logs\")\n",
    "    runs.sort()\n",
    "    while True:\n",
    "        latest_run = runs.pop()\n",
    "        if path.isdir(f\"./logs/{latest_run}\") and RECIPE_NAME in latest_run:\n",
    "            break\n",
    "    latest_run = \"all_run-20240614-041748\"\n",
    "    modelPath = f\"./logs/{latest_run}/weights/best.pt\"\n",
    "    print(f\"Latest run: {latest_run}, loading model from {modelPath}\")\n",
    "    bestModel = YOLO(modelPath).to('cuda')\n",
    "    model = bestModel\n",
    "    print(\"Loaded best model\")\n",
    "\n",
    "while cv2.waitKey(0) != ord('q'):\n",
    "    cv2.destroyAllWindows()\n",
    "    randomFile = random.choice(listdir(DATA))\n",
    "    # randomFile = \"obb_resistor_yellow_violet_black_gold_brown_470E-1-1_30.png\"\n",
    "    print(f\"Random file: {randomFile}\")\n",
    "    results = model.predict(f\"{DATA}/{randomFile}\", show=True)\n",
    "    if len(results[0].obb.xyxyxyxy) > 0:\n",
    "        cropped_img = crop_image_to_bbox(results[0].orig_img, results[0].obb.xyxyxyxy)\n",
    "        cv2.imshow(\"Cropped Image\", cropped_img)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw vertices to verify direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random file: obb_resistor_orange_white_black_brown_brown_390E1-1_15.png\n",
      "\n",
      "image 1/1 c:\\Users\\Shaheen\\OneDrive - Imperial College London\\Uni\\CW Labs\\Year 4\\FYP\\src\\vision\\datasets\\full\\resistor\\imgs\\obb_resistor_orange_white_black_brown_brown_390E1-1_15.png: 480x640 53.0ms\n",
      "Speed: 5.5ms preprocess, 53.0ms inference, 5.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shaheen\\AppData\\Local\\Temp\\ipykernel_30816\\1325440353.py:39: DeprecationWarning: `np.int0` is a deprecated alias for `np.intp`.  (Deprecated NumPy 1.24)\n",
      "  box = np.array(bbox[0].cpu(), dtype=np.int0)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def draw_bounding_box(image, bbox, bbox_format, color=(0, 255, 0), thickness=2):\n",
    "    \"\"\"\n",
    "    Draws a bounding box on the image with an arrow indicating orientation for the xyxyxyxy format.\n",
    "\n",
    "    Args:\n",
    "    - image (np.array): The input image.\n",
    "    - bbox (tensor): The bounding box coordinates.\n",
    "    - bbox_format (str): The format of the bounding box ('xywhr', 'xyxy', 'xyxyxyxy', 'xyxyxyxyn').\n",
    "    - color (tuple): The color of the bounding box (default is green).\n",
    "    - thickness (int): The thickness of the bounding box lines (default is 2).\n",
    "\n",
    "    Returns:\n",
    "    - image_with_box (np.array): The image with the bounding box drawn.\n",
    "    \"\"\"\n",
    "    image_with_box = image.copy()  # Make a copy of the image to avoid overwriting\n",
    "\n",
    "    if bbox_format == 'xywhr':\n",
    "        # xywhr: [x_center, y_center, width, height, rotation (in radians)]\n",
    "        x_center, y_center, width, height, rotation = bbox[0]\n",
    "        center = (int(x_center.item()), int(y_center.item()))\n",
    "        size = (int(width.item()), int(height.item()))\n",
    "        angle = np.degrees(rotation.item())\n",
    "\n",
    "        rect = ((center[0], center[1]), (size[0], size[1]), angle)\n",
    "        box = cv2.boxPoints(rect)\n",
    "        box = np.int0(box)\n",
    "\n",
    "    elif bbox_format == 'xyxy':\n",
    "        # xyxy: [x_min, y_min, x_max, y_max]\n",
    "        x_min, y_min, x_max, y_max = bbox[0]\n",
    "        box = np.array([[x_min.item(), y_min.item()],\n",
    "                        [x_max.item(), y_min.item()],\n",
    "                        [x_max.item(), y_max.item()],\n",
    "                        [x_min.item(), y_max.item()]], dtype=np.int0)\n",
    "\n",
    "    elif bbox_format == 'xyxyxyxy':\n",
    "        # xyxyxyxy: [[x1, y1], [x2, y2], [x3, y3], [x4, y4]]\n",
    "        box = np.array(bbox[0].cpu(), dtype=np.int0)\n",
    "\n",
    "    elif bbox_format == 'xyxyxyxyn':\n",
    "        # xyxyxyxyn: normalized [[x1, y1], [x2, y2], [x3, y3], [x4, y4]]\n",
    "        h, w = image.shape[:2]\n",
    "        box = np.array(bbox[0].cpu() * np.array([w, h]), dtype=np.int0)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported bbox_format: {bbox_format}\")\n",
    "\n",
    "    # Draw the polygon\n",
    "    image_with_box = cv2.polylines(image_with_box, [box], isClosed=True, color=color, thickness=thickness)\n",
    "\n",
    "    # Draw an arrow for the xyxyxyxy format to indicate orientation\n",
    "    if bbox_format == 'xyxyxyxy':\n",
    "        start_point = (int(box[0][0]), int(box[0][1]))\n",
    "        end_point = (int(box[1][0]), int(box[1][1]))\n",
    "        image_with_box = cv2.arrowedLine(image_with_box, start_point, end_point, color, thickness, tipLength=0.3)\n",
    "\n",
    "     # Number each vertex\n",
    "    for i, (x, y) in enumerate(box):\n",
    "        cv2.putText(image_with_box, str(i+1), (x, y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), thickness, cv2.LINE_AA)\n",
    "\n",
    "    return image_with_box\n",
    "\n",
    "DATA = PATHS['resistors']\n",
    "randomFile = random.choice(listdir(DATA))\n",
    "print(f\"Random file: {randomFile}\")\n",
    "results = model.predict(f\"{DATA}/{randomFile}\", show=True)\n",
    "\n",
    "cv2.imshow(randomFile, draw_bounding_box(results[0].orig_img, results[0].obb.xyxyxyxy, 'xyxyxyxy'))\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export model to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'logs\\all_run-20240610-045510\\weights\\best.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 16, 8400) (6.4 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.14.1 opset 13...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success  1.0s, saved as 'logs\\all_run-20240610-045510\\weights\\best.onnx' (11.9 MB)\n",
      "\n",
      "Export complete (1.2s)\n",
      "Results saved to \u001b[1mC:\\Users\\Shaheen\\OneDrive - Imperial College London\\Uni\\CW Labs\\Year 4\\FYP\\src\\vision\\logs\\all_run-20240610-045510\\weights\u001b[0m\n",
      "Predict:         yolo predict task=obb model=logs\\all_run-20240610-045510\\weights\\best.onnx imgsz=640  \n",
      "Validate:        yolo val task=obb model=logs\\all_run-20240610-045510\\weights\\best.onnx imgsz=640 data=./models/recipes/dataset_desc.yaml  \n",
      "Visualize:       https://netron.app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'logs\\\\all_run-20240610-045510\\\\weights\\\\best.onnx'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.export(format=\"onnx\", opset=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random file: obb_ceramic_capacitor_472k_1.png\n",
      "Output shape: (1, 16, 8400)\n",
      "Filtered boxes: 0\n",
      "Random file: obb_resistor_green_brown_black_red_brown_510E2-1_1.png\n",
      "Output shape: (1, 16, 8400)\n",
      "Filtered boxes: 10\n",
      "Random file: obb_resistor_orange_blue_black_gold_brown_gold_360E-1-1_14.png\n",
      "Output shape: (1, 16, 8400)\n",
      "Filtered boxes: 12\n",
      "Random file: obb_ceramic_capacitor_10_6.png\n",
      "Output shape: (1, 16, 8400)\n",
      "Filtered boxes: 0\n",
      "Random file: obb_led_yellow_4.png\n",
      "Output shape: (1, 16, 8400)\n",
      "Filtered boxes: 0\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "from os import listdir\n",
    "\n",
    "\n",
    "# Create the ONNX inference session\n",
    "onnxModel = ort.InferenceSession(f\"./logs\\\\all_run-20240610-045510\\\\weights\\\\best.onnx\")\n",
    "CONFIDENCE_THRESHOLD = 0.1\n",
    "\n",
    "def preprocess(image):\n",
    "    input_shape = onnxModel.get_inputs()[0].shape  # get input shape\n",
    "    input_height, input_width = input_shape[2], input_shape[3]\n",
    "\n",
    "    image = cv2.resize(image, (input_width, input_height))\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # convert to RGB\n",
    "    image = image.astype(np.float32)\n",
    "    image = image / 255.0  # normalize to [0, 1]\n",
    "    image = np.transpose(image, (2, 0, 1))  # convert to CHW\n",
    "    image = np.expand_dims(image, axis=0)  # add batch dimension\n",
    "\n",
    "    return image\n",
    "\n",
    "def postprocess(output, original_image):\n",
    "    # Debugging: print the shape of the output\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "\n",
    "    output = np.squeeze(output)  # remove batch dimension\n",
    "\n",
    "    boxes = []\n",
    "    for i in range(output.shape[1]):\n",
    "        box = output[:, i]\n",
    "        x_center, y_center, width, height, objectness = box[:5]\n",
    "        if objectness > CONFIDENCE_THRESHOLD:\n",
    "            x_center *= original_image.shape[1]  # scale back to original image size\n",
    "            y_center *= original_image.shape[0]\n",
    "            width *= original_image.shape[1]\n",
    "            height *= original_image.shape[0]\n",
    "\n",
    "            x_min = int(x_center - width / 2)\n",
    "            y_min = int(y_center - height / 2)\n",
    "            x_max = int(x_center + width / 2)\n",
    "            y_max = int(y_center + height / 2)\n",
    "            boxes.append((x_min, y_min, x_max, y_max))\n",
    "    \n",
    "    print(f\"Filtered boxes: {len(boxes)}\")\n",
    "\n",
    "    for box in boxes:\n",
    "        cv2.rectangle(original_image, (box[0], box[1]), (box[2], box[3]), (0, 255, 0), 2)\n",
    "\n",
    "    return original_image\n",
    "\n",
    "# Fetch the actual input name from the model\n",
    "input_name = onnxModel.get_inputs()[0].name\n",
    "\n",
    "while cv2.waitKey(0) != ord('q'):\n",
    "    cv2.destroyAllWindows()\n",
    "    randomFile = random.choice(listdir(DATA))\n",
    "    print(f\"Random file: {randomFile}\")\n",
    "\n",
    "    image = cv2.imread(f\"{DATA}/{randomFile}\")\n",
    "    input_tensor = preprocess(image)\n",
    "\n",
    "    # Perform inference\n",
    "    results = onnxModel.run(None, {input_name: input_tensor})[0]\n",
    "    output_image = postprocess(results, image.copy())\n",
    "\n",
    "    cv2.imshow(randomFile, output_image)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[     5.2338      17.409      26.986 ...      510.89       548.1      579.53]\n",
      "  [     5.7821       5.787      7.7035 ...      599.64      588.09      583.31]\n",
      "  [     11.832       24.92      39.459 ...      238.35      186.63      154.18]\n",
      "  ...\n",
      "  [ 3.8743e-07  5.6624e-07  5.6624e-07 ...    4.53e-06  4.2915e-06  4.4405e-06]\n",
      "  [  1.809e-05  1.0133e-05  7.6294e-06 ...  1.4275e-05  1.7732e-05  3.8892e-05]\n",
      "  [  0.0060101    0.029564    0.015276 ...  -0.0082752  -0.0093197    0.017739]]]\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random file: obb_resistor_yellow_violet_black_gold_brown_470E-1-1_30.png\n",
      "\n",
      "image 1/1 c:\\Users\\Shaheen\\OneDrive - Imperial College London\\Uni\\CW Labs\\Year 4\\FYP\\src\\vision\\datasets\\full\\current\\images\\test\\obb_resistor_yellow_violet_black_gold_brown_470E-1-1_30.png: 480x640 47.5ms\n",
      "Speed: 3.5ms preprocess, 47.5ms inference, 7.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[377 153]\n"
     ]
    }
   ],
   "source": [
    "while cv2.waitKey(0) != ord('q'):\n",
    "    cv2.destroyAllWindows()\n",
    "    randomFile = random.choice(listdir(DATA))\n",
    "    randomFile = \"obb_resistor_yellow_violet_black_gold_brown_470E-1-1_30.png\"\n",
    "    print(f\"Random file: {randomFile}\")\n",
    "    results = model.predict(f\"{DATA}/{randomFile}\", show=True)\n",
    "    cropped_img = crop_image_to_bbox(results[0].orig_img, results[0].obb.xyxyxyxy)\n",
    "    cv2.imshow(\"Cropped Image\", cropped_img)\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "from os import path, listdir, makedirs\n",
    "from shutil import rmtree, copy\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the full classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "IMAGE_SIZE = 640 # x 480\n",
    "PROJECT_NAME = \"logs\"\n",
    "RECIPE_NAME = \"all\"\n",
    "TRAIN_RUN = f\"{RECIPE_NAME}_run-{time.strftime('%Y%m%d-%H%M%S')}\"\n",
    "DATA_PATH = \"./../../datasets/full\"\n",
    "\n",
    "# Model\n",
    "MODEL_PATH = \"./models/pretrained/yolov8n-obb-dotav1.pt\"\n",
    "TEST_PATH = \"./models/recipes/dataset_desc_test.yaml\"\n",
    "VAL_PATH = \"./models/recipes/dataset_desc.yaml\"\n",
    "VALTEST_PATH = \"./models/recipes/dataset_desc_valtest.yaml\"\n",
    "TRAIN_PARAM = {\n",
    "    # Model definition\n",
    "    'data': VAL_PATH,\n",
    "    'resume': False,\n",
    "    'device': '0',\n",
    "    'pretrained': True,\n",
    "    # Names\n",
    "    'project' : PROJECT_NAME,\n",
    "    'name': TRAIN_RUN,\n",
    "    # Training Parameters\n",
    "    'batch': -1,\n",
    "    'imgsz': IMAGE_SIZE,\n",
    "    'epochs': 50,\n",
    "    'patience': 5,\n",
    "    'cos_lr': True,\n",
    "    # \"lr0\": 0.05,\n",
    "    # Augmentation\n",
    "    'hsv_h': 0.05, # Higher than default for resistor\n",
    "    'hsv_s': 0.3, # Colours should not change too much\n",
    "    'hsv_v': 0.2, # Colours should not change too much\n",
    "    'degrees': 180, # Rotation\n",
    "    'translate': 0.1, # Translation\n",
    "    'scale': 0.8, # Scaling - camera is always at the same distance\n",
    "    'shear': 10.0, # Shearing\n",
    "    'perspective': 0.0, # Perspective\n",
    "    'flipud': 0.5, # Flip up-down\n",
    "    'fliplr': 0.5, # Flip left-right\n",
    "    'mosaic': 0.5, # Mosaic\n",
    "    'mixup': 0.0, # Mixup\n",
    "    'copy_paste': 0.0, # Copy-paste\n",
    "    'crop_fraction': 1.0, # Crop fraction\n",
    "    # Loss weights\n",
    "    # 'cls' : 1.0, # Class\n",
    "    # 'box' : 4.0, # Box accuracy\n",
    "    # 'dfl' : 1.5, # Help manage unbalanced classes\n",
    "    # Post parameters\n",
    "    'save': True,\n",
    "    'save_period': 5,\n",
    "    'plots': False,\n",
    "    # Misc\n",
    "    'verbose': False,\n",
    "}\n",
    "\n",
    "PATHS = {\n",
    "    'train': f\"{DATA_PATH}/current/images/train\",\n",
    "    'val': f\"{DATA_PATH}/current/images/val\",\n",
    "    'test': f\"{DATA_PATH}/current/images/test\",\n",
    "    'resistors' : f\"{DATA_PATH}/resistor/imgs\",\n",
    "    'ceramic_cap' : f\"{DATA_PATH}/ceramic_capacitor/imgs\",\n",
    "    \"none\" : \"./datasets/none\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 164 images and 164 labels in capacitor\n",
      "Found 264 images and 264 labels in ceramic_capacitor\n",
      "Found 66 images and 66 labels in film_capacitor\n",
      "Found 52 images and 52 labels in inductor\n",
      "Found 96 images and 96 labels in led\n",
      "Found 258 images and 259 labels in resistor\n",
      "Found 75 images and 75 labels in wire\n",
      "Split into 682 train, 195 val, 97 test. Total: 974 images.\n"
     ]
    }
   ],
   "source": [
    "# Reorganise the data\n",
    "PARAM = {\n",
    "    'path' : DATA_PATH,\n",
    "    'train' : 0.7,\n",
    "    'val' : 0.2,\n",
    "    'test' : 0.1,\n",
    "    'include' : [\n",
    "        'resistor',\n",
    "        'capacitor',\n",
    "        'ceramic_capacitor',\n",
    "        'film_capacitor',\n",
    "        'inductor',\n",
    "        'led',\n",
    "        'wire'\n",
    "    ]\n",
    "}\n",
    "# Remove old dataset\n",
    "DATASET_FOLDER = f\"{DATA_PATH}/current\"\n",
    "if path.exists(DATASET_FOLDER):\n",
    "    for folder in listdir(DATASET_FOLDER):\n",
    "        rmtree(path.join(DATASET_FOLDER, folder), ignore_errors=True)\n",
    "    # Make label folder\n",
    "    makedirs(path.join(DATASET_FOLDER, 'labels'), exist_ok=True)\n",
    "    # Make train, val, test folders\n",
    "    for folder in ['train', 'val', 'test']:\n",
    "        makedirs(path.join(DATASET_FOLDER, 'images', folder), exist_ok=True)\n",
    "        makedirs(path.join(DATASET_FOLDER, 'labels', folder), exist_ok=True)\n",
    "\n",
    "# Get all component images from all folders\n",
    "basenames = []\n",
    "# For component folder in dataset\n",
    "for folder in listdir(PARAM['path']):\n",
    "    # Skip if not in include\n",
    "    if folder not in PARAM['include']: continue\n",
    "    # For each subfolder in component folder\n",
    "    imgfiles = listdir(path.join(PARAM['path'], folder, 'imgs'))\n",
    "    labfiles = listdir(path.join(PARAM['path'], folder, 'labels'))\n",
    "    bases = [path.join(folder, 'imgs', path.splitext(f)[0]) for f in imgfiles]\n",
    "    basenames.extend(bases)\n",
    "    print(f\"Found {len(imgfiles)} images and {len(labfiles)} labels in {folder}\")\n",
    "\n",
    "# Split the data into train, val, test\n",
    "random.shuffle(basenames)\n",
    "train = int(len(basenames) * PARAM['train'])\n",
    "val = int(len(basenames) * PARAM['val'])\n",
    "test = int(len(basenames) * PARAM['test'])\n",
    "print(f\"Split into {train} train, {val} val, {test} test. Total: {train+val+test} images.\")\n",
    "train_set = basenames[:train]\n",
    "val_set = basenames[train:train+val]\n",
    "test_set = basenames[train+val:]\n",
    "\n",
    "# Copy the images and labels to the new dataset folder\n",
    "for folder, dataset in zip(['train', 'val', 'test'], [train_set, val_set, test_set]):\n",
    "    for base in dataset:\n",
    "        filename = path.split(base)[1]\n",
    "        copy(path.join(PARAM['path'], f\"{base}.png\"), path.join(DATASET_FOLDER, 'images', folder, f\"{filename}.png\"))\n",
    "        base = base.replace('imgs', 'labels')\n",
    "        copy(path.join(PARAM['path'], f\"{base}.txt\"), path.join(DATASET_FOLDER, 'labels', folder, f\"{filename}.txt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-a12bdda2596e78e6\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-a12bdda2596e78e6\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tensorboard logging\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir \"logs\" --port=6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: The process \"9424\" not found.\n",
      "ERROR: The process \"tensorboard.exe\" not found.\n"
     ]
    }
   ],
   "source": [
    "! taskkill /PID 9424 /F\n",
    "! taskkill /IM \"tensorboard.exe\" /F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.2.32 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.2.31  Python-3.10.13 torch-2.1.1+cu121 CUDA:0 (NVIDIA GeForce RTX 3080 Laptop GPU, 16384MiB)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=obb, mode=train, model=./models/pretrained/yolov8n-obb-dotav1.pt, data=./models/recipes/dataset_desc.yaml, epochs=50, time=None, patience=5, batch=-1, imgsz=640, save=True, save_period=5, cache=False, device=0, workers=8, project=logs, name=all_run-20240614-0444198, exist_ok=False, pretrained=True, optimizer=auto, verbose=False, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=True, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=False, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.05, hsv_s=0.3, hsv_v=0.2, degrees=180, translate=0.1, scale=0.8, shear=10.0, perspective=0.0, flipud=0.5, fliplr=0.5, bgr=0.0, mosaic=0.5, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=logs\\all_run-20240614-0444198\n",
      "Overriding model.yaml nc=15 with nc=11\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    825124  ultralytics.nn.modules.head.OBB              [11, 1, [64, 128, 256]]       \n",
      "YOLOv8n-obb summary: 250 layers, 3084660 parameters, 3084644 gradients, 8.5 GFLOPs\n",
      "\n",
      "Transferred 391/397 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir logs\\all_run-20240614-0444198', view at http://localhost:6006/\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n",
      "\u001b[34m\u001b[1mAutoBatch: \u001b[0mComputing optimal batch size for imgsz=640 at 60.0% CUDA memory utilization.\n",
      "\u001b[34m\u001b[1mAutoBatch: \u001b[0mCUDA:0 (NVIDIA GeForce RTX 3080 Laptop GPU) 16.00G total, 1.35G reserved, 0.17G allocated, 14.49G free\n",
      "      Params      GFLOPs  GPU_mem (GB)  forward (ms) backward (ms)                   input                  output\n",
      "     3084660       8.452         1.581            72           nan        (1, 3, 640, 640)                    list\n",
      "     3084660        16.9         0.786         22.17           nan        (2, 3, 640, 640)                    list\n",
      "     3084660       33.81         1.191          22.5           nan        (4, 3, 640, 640)                    list\n",
      "     3084660       67.62         2.112            26           nan        (8, 3, 640, 640)                    list\n",
      "     3084660       135.2         3.641          32.5           nan       (16, 3, 640, 640)                    list\n",
      "\u001b[34m\u001b[1mAutoBatch: \u001b[0mUsing batch-size 46 for CUDA:0 10.15G/16.00G (63%) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\Shaheen\\OneDrive - Imperial College London\\Uni\\CW Labs\\Year 4\\FYP\\datasets\\full\\current\\labels\\train... 682 images, 72 backgrounds, 0 corrupt: 100%|██████████| 754/754 [00:00<00:00, 1300.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: C:\\Users\\Shaheen\\OneDrive - Imperial College London\\Uni\\CW Labs\\Year 4\\FYP\\datasets\\full\\current\\labels\\train.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Shaheen\\OneDrive - Imperial College London\\Uni\\CW Labs\\Year 4\\FYP\\datasets\\full\\current\\labels\\val.cache... 195 images, 20 backgrounds, 0 corrupt: 100%|██████████| 215/215 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000667, momentum=0.9) with parameter groups 63 weight(decay=0.0), 73 weight(decay=0.000359375), 72 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added \n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mlogs\\all_run-20240614-0444198\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/50      6.62G      1.942      5.735      2.465         18        640: 100%|██████████| 17/17 [00:13<00:00,  1.23it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:04<00:00,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        215        195      0.421      0.148       0.17      0.112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/50      6.53G      1.345      3.862      1.738         23        640: 100%|██████████| 17/17 [00:05<00:00,  3.24it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:02<00:00,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        215        195      0.371      0.596      0.508      0.349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/50      6.53G       1.18      2.326      1.563         20        640: 100%|██████████| 17/17 [00:06<00:00,  2.79it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  2.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        215        195      0.503      0.727      0.654      0.483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/50      6.52G      1.134      1.805      1.516         25        640: 100%|██████████| 17/17 [00:06<00:00,  2.79it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:04<00:00,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        215        195      0.807      0.853      0.899       0.68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/50      6.52G      1.116      1.382      1.525         23        640: 100%|██████████| 17/17 [00:05<00:00,  3.01it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  2.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        215        195      0.922      0.979      0.968      0.719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/50      6.52G      1.126      1.155      1.559         29        640: 100%|██████████| 17/17 [00:05<00:00,  3.23it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  3.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        215        195      0.886      0.886       0.94       0.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/50      6.52G       1.11      1.031      1.559         32        640: 100%|██████████| 17/17 [00:05<00:00,  3.32it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  2.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        215        195      0.898      0.923      0.963      0.708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/50      6.52G      1.118     0.9605      1.602         24        640: 100%|██████████| 17/17 [00:04<00:00,  3.44it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  2.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        215        195      0.889      0.923      0.974      0.735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/50      6.52G      1.077      0.863      1.553         28        640: 100%|██████████| 17/17 [00:04<00:00,  3.49it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  3.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        215        195      0.894      0.896      0.962      0.743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/50      6.52G      1.065     0.8296       1.54         29        640: 100%|██████████| 17/17 [00:04<00:00,  3.44it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  3.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        215        195      0.866      0.939      0.967      0.742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      11/50      6.52G      1.066     0.7703      1.566         18        640: 100%|██████████| 17/17 [00:04<00:00,  3.42it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  3.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        215        195      0.969       0.98      0.981      0.739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      12/50      6.52G      1.061     0.7225      1.544         24        640: 100%|██████████| 17/17 [00:04<00:00,  3.48it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  3.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        215        195      0.951      0.986      0.986       0.74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      13/50      6.52G      1.062     0.7067      1.575         28        640: 100%|██████████| 17/17 [00:04<00:00,  3.43it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  3.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        215        195      0.948      0.967      0.965       0.76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      14/50      6.52G      1.054     0.6881       1.58         17        640: 100%|██████████| 17/17 [00:05<00:00,  3.40it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  3.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        215        195      0.978      0.987      0.992      0.756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      15/50      6.52G      1.023     0.6738      1.556         21        640: 100%|██████████| 17/17 [00:05<00:00,  3.40it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  3.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        215        195      0.983      0.946      0.993      0.772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      16/50      6.52G      1.043      0.682      1.599         27        640: 100%|██████████| 17/17 [00:04<00:00,  3.45it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  2.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        215        195      0.979      0.983      0.983      0.762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      17/50      6.52G      1.009     0.6459      1.511         24        640: 100%|██████████| 17/17 [00:05<00:00,  3.34it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  3.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        215        195      0.943      0.997      0.989      0.755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      18/50      6.52G      1.017     0.6549      1.565         24        640: 100%|██████████| 17/17 [00:05<00:00,  3.40it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  2.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        215        195      0.993      0.995      0.995      0.796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      19/50      6.52G          1     0.6065      1.486         28        640: 100%|██████████| 17/17 [00:05<00:00,  3.25it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  3.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        215        195      0.993       0.98      0.994      0.793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      20/50      6.52G      1.001     0.5994      1.548         22        640: 100%|██████████| 17/17 [00:05<00:00,  3.37it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  3.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        215        195      0.979      0.987      0.993      0.792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      21/50      6.52G     0.9795     0.6059      1.493         20        640: 100%|██████████| 17/17 [00:05<00:00,  3.27it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  2.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        215        195      0.977      0.997      0.995      0.779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      22/50      6.52G     0.9604     0.5724       1.48         27        640: 100%|██████████| 17/17 [00:05<00:00,  3.05it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  2.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        215        195      0.992       0.99      0.995      0.803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      23/50      6.52G     0.9849     0.5826      1.538         20        640: 100%|██████████| 17/17 [00:05<00:00,  3.31it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  2.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        215        195      0.982          1      0.995      0.791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      24/50      6.52G     0.9741     0.5543      1.534         23        640: 100%|██████████| 17/17 [00:05<00:00,  3.18it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  2.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        215        195      0.978          1      0.995       0.79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      25/50      6.52G     0.9674     0.5732      1.523         25        640: 100%|██████████| 17/17 [00:05<00:00,  3.18it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  3.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        215        195      0.971      0.995      0.995      0.766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      26/50      6.52G     0.9699     0.5678      1.542         21        640: 100%|██████████| 17/17 [00:05<00:00,  3.29it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  3.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        215        195      0.976      0.997      0.995      0.804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      27/50      6.52G     0.9643     0.5441      1.549         21        640: 100%|██████████| 17/17 [00:05<00:00,  3.21it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  2.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        215        195      0.983          1      0.995      0.797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      28/50      6.52G     0.9553     0.5329      1.492         26        640: 100%|██████████| 17/17 [00:05<00:00,  3.19it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  3.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        215        195      0.986      0.998      0.995      0.798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      29/50      6.52G     0.9526     0.5335      1.513         22        640: 100%|██████████| 17/17 [00:05<00:00,  3.26it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  3.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        215        195      0.986          1      0.995       0.81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      30/50      6.52G     0.9632     0.5431      1.511         21        640: 100%|██████████| 17/17 [00:05<00:00,  3.20it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  3.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        215        195      0.987          1      0.995      0.804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      31/50      6.52G     0.9322     0.5274      1.521         25        640: 100%|██████████| 17/17 [00:05<00:00,  3.33it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  2.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        215        195       0.99      0.998      0.995      0.806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      32/50      6.52G     0.9478     0.5342      1.516         22        640: 100%|██████████| 17/17 [00:05<00:00,  3.30it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  2.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        215        195      0.987      0.998      0.995      0.816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      33/50      6.52G     0.9389     0.5374      1.509         27        640: 100%|██████████| 17/17 [00:05<00:00,  3.18it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  3.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        215        195      0.984      0.997      0.995      0.816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      34/50      6.52G       0.91     0.5011      1.488         20        640: 100%|██████████| 17/17 [00:05<00:00,  3.21it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  2.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        215        195      0.989      0.998      0.995      0.826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      35/50      6.52G     0.9366      0.528      1.479         17        640: 100%|██████████| 17/17 [00:05<00:00,  3.30it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  3.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        215        195      0.987          1      0.995      0.828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      36/50      6.52G     0.9354     0.5129      1.497         21        640: 100%|██████████| 17/17 [00:05<00:00,  3.32it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  3.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        215        195      0.983          1      0.995       0.81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      37/50      6.52G     0.9294     0.5177      1.477         28        640: 100%|██████████| 17/17 [00:05<00:00,  3.38it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  3.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        215        195      0.982          1      0.995      0.826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      38/50      6.52G     0.9183     0.4933      1.487         22        640: 100%|██████████| 17/17 [00:05<00:00,  3.25it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  2.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        215        195      0.982          1      0.995      0.825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      39/50      6.52G     0.8882     0.4756      1.457         22        640: 100%|██████████| 17/17 [00:05<00:00,  3.25it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  2.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        215        195      0.983          1      0.995      0.824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      40/50      6.52G     0.9196     0.4826      1.496         22        640: 100%|██████████| 17/17 [00:05<00:00,  3.31it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  3.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        215        195      0.984      0.999      0.995      0.825\n",
      "\u001b[34m\u001b[1mEarlyStopping: \u001b[0mTraining stopped early as no improvement observed in last 5 epochs. Best results observed at epoch 35, best model saved as best.pt.\n",
      "To update EarlyStopping(patience=5) pass a new patience value, i.e. `patience=300` or use `patience=0` to disable EarlyStopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "40 epochs completed in 0.095 hours.\n",
      "Optimizer stripped from logs\\all_run-20240614-0444198\\weights\\last.pt, 6.9MB\n",
      "Optimizer stripped from logs\\all_run-20240614-0444198\\weights\\best.pt, 6.9MB\n",
      "\n",
      "Validating logs\\all_run-20240614-0444198\\weights\\best.pt...\n",
      "Ultralytics YOLOv8.2.31  Python-3.10.13 torch-2.1.1+cu121 CUDA:0 (NVIDIA GeForce RTX 3080 Laptop GPU, 16384MiB)\n",
      "YOLOv8n-obb summary (fused): 187 layers, 3079364 parameters, 0 gradients, 8.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  3.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        215        195      0.987          1      0.995      0.829\n",
      "Speed: 0.2ms preprocess, 1.4ms inference, 0.0ms loss, 2.4ms postprocess per image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ultralytics.utils.metrics.OBBMetrics object with attributes:\n",
       "\n",
       "ap_class_index: array([ 0,  1,  2,  3,  7,  8, 10])\n",
       "box: ultralytics.utils.metrics.Metric object\n",
       "confusion_matrix: <ultralytics.utils.metrics.ConfusionMatrix object at 0x000000007FF438B0>\n",
       "curves: []\n",
       "curves_results: []\n",
       "fitness: 0.8452807931944819\n",
       "keys: ['metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)']\n",
       "maps: array([    0.80864,     0.87753,     0.80953,      0.8877,     0.82865,     0.82865,     0.82865,     0.88781,     0.67608,     0.82865,     0.85323])\n",
       "names: {0: \"{0: 'resistor'}\", 1: \"{1: 'capacitor'}\", 2: \"{2: 'ceramic_cap'}\", 3: \"{3: 'inductors'}\", 4: \"{4: 'diodes'}\", 5: \"{5: 'mosfet'}\", 6: \"{6: 'transistor'}\", 7: \"{7: 'leds'}\", 8: \"{8: 'wire'}\", 9: \"{9: 'ics'}\", 10: \"{10: 'film_cap'}\"}\n",
       "plot: False\n",
       "results_dict: {'metrics/precision(B)': 0.9869659839494052, 'metrics/recall(B)': 0.9997427080760415, 'metrics/mAP50(B)': 0.995, 'metrics/mAP50-95(B)': 0.8286453257716465, 'fitness': 0.8452807931944819}\n",
       "save_dir: WindowsPath('logs/all_run-20240614-0444198')\n",
       "speed: {'preprocess': 0.21160480587981467, 'inference': 1.3511702071788698, 'loss': 0.0, 'postprocess': 2.353558429451876}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Raw model\n",
    "# model = YOLO(\"yolov8n-obb.yaml\").to('cuda')\n",
    "\n",
    "# Train\n",
    "model = YOLO(MODEL_PATH).to('cuda')\n",
    "model.train(**TRAIN_PARAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Shaheen\\OneDrive - Imperial College London\\Uni\\CW Labs\\Year 4\\FYP\\datasets\\full\\current\\labels\\valtest...:   0%|          | 0/323 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Shaheen\\OneDrive - Imperial College London\\Uni\\CW Labs\\Year 4\\FYP\\datasets\\full\\current\\labels\\valtest... 293 images, 30 backgrounds, 0 corrupt: 100%|██████████| 323/323 [00:00<00:00, 584.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: C:\\Users\\Shaheen\\OneDrive - Imperial College London\\Uni\\CW Labs\\Year 4\\FYP\\datasets\\full\\current\\labels\\valtest.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 21/21 [00:04<00:00,  4.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        323        293      0.988      0.999      0.995      0.817\n",
      "       {0: 'resistor'}         76         76      0.997          1      0.995      0.806\n",
      "      {1: 'capacitor'}         43         43      0.997          1      0.995      0.862\n",
      "    {2: 'ceramic_cap'}         84         84       0.98          1      0.995      0.798\n",
      "      {3: 'inductors'}         23         23       0.99          1      0.995      0.885\n",
      "           {7: 'leds'}         34         34      0.993          1      0.995      0.884\n",
      "           {8: 'wire'}         13         13          1      0.993      0.995      0.646\n",
      "      {10: 'film_cap'}         20         20      0.957          1      0.995      0.839\n",
      "Speed: 0.6ms preprocess, 3.0ms inference, 0.0ms loss, 2.6ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\obb\\val15\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "metrics = model.val(data=VALTEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[         76           0           0           0           0           0           0           0           0           0           0           0]\n",
      " [          0          43           0           0           0           0           0           0           0           0           0           0]\n",
      " [          0           0          84           0           0           0           0           0           0           0           0           0]\n",
      " [          0           0           0          23           0           0           0           0           0           0           0           0]\n",
      " [          0           0           0           0           0           0           0           0           0           0           0           0]\n",
      " [          0           0           0           0           0           0           0           0           0           0           0           0]\n",
      " [          0           0           0           0           0           0           0           0           0           0           0           0]\n",
      " [          0           0           0           0           0           0           0          34           0           0           0           0]\n",
      " [          0           0           0           0           0           0           0           0          13           0           0           0]\n",
      " [          0           0           0           0           0           0           0           0           0           0           0           0]\n",
      " [          0           0           0           0           0           0           0           0           0           0          20           0]\n",
      " [          0           0           0           0           0           0           0           0           0           0           0           0]]\n",
      "[[         76           0           0           0           0           0           0           0]\n",
      " [          0          43           0           0           0           0           0           0]\n",
      " [          0           0          84           0           0           0           0           0]\n",
      " [          0           0           0          23           0           0           0           0]\n",
      " [          0           0           0           0          34           0           0           0]\n",
      " [          0           0           0           0           0          13           0           0]\n",
      " [          0           0           0           0           0           0          20           0]\n",
      " [          0           0           0           0           0           0           0           0]]\n"
     ]
    }
   ],
   "source": [
    "confusion_matrix = metrics.confusion_matrix.matrix\n",
    "\n",
    "remove = [4, 5, 6, 9]\n",
    "filtered_matrix = np.delete(confusion_matrix, remove, axis=0)\n",
    "filtered_matrix = np.delete(filtered_matrix, remove, axis=1)\n",
    "print(confusion_matrix)\n",
    "print(filtered_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "NORMALISE = False\n",
    "metrics.confusion_matrix.matrix = filtered_matrix\n",
    "metrics.confusion_matrix.nc = 7\n",
    "metrics.confusion_matrix.plot(names=('resistor', 'capacitor', 'ceramic_capacitor', \"inductor\", \"led\", \"wire\", \"film_capacitor\"), normalize=NORMALISE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grab component in box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image_to_bbox(image, bbox):\n",
    "    \"\"\"\n",
    "    Crop the image to the region within the bounding box.\n",
    "\n",
    "    Args:\n",
    "    - image (np.array): The input image.\n",
    "    - bbox (tensor): The coordinates of the bounding box in xyxyxyxy format.\n",
    "\n",
    "    Returns:\n",
    "    - cropped_image (np.array): The cropped image.\n",
    "    \"\"\"\n",
    "    # Convert tensor to numpy array and ensure it's on the CPU\n",
    "    bbox = bbox.cpu().numpy()[0]\n",
    "\n",
    "    # Compute the width and height of the bounding box\n",
    "    width = int(np.linalg.norm(bbox[0] - bbox[1]))\n",
    "    height = int(np.linalg.norm(bbox[1] - bbox[2]))\n",
    "\n",
    "    # Compute the center of the bounding box\n",
    "    center = np.mean(bbox, axis=0).astype(int)\n",
    "\n",
    "    # Compute the rotation angle of the bounding box\n",
    "    angle = np.degrees(np.arctan2(bbox[1, 1] - bbox[0, 1], bbox[1, 0] - bbox[0, 0]))\n",
    "    rotation_matrix = cv2.getRotationMatrix2D((int(center[0]), int(center[1])), angle, 1.0)\n",
    "\n",
    "    # Apply the rotation to the image\n",
    "    rotated_image = cv2.warpAffine(image, rotation_matrix, (image.shape[1], image.shape[0]))\n",
    "\n",
    "    # Get the bounding box in the rotated image\n",
    "    x, y = center - [width // 2, height // 2]\n",
    "    cropped_image = rotated_image[y:y+height, x:x+width]\n",
    "\n",
    "    return cropped_image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded trained model\n",
      "Random file: obb_ceramic_capacitor_10_6.png\n",
      "\n",
      "image 1/1 c:\\Users\\Shaheen\\OneDrive - Imperial College London\\Uni\\CW Labs\\Year 4\\FYP\\src\\vision\\..\\..\\datasets\\full\\current\\images\\test\\obb_ceramic_capacitor_10_6.png: 480x640 85.5ms\n",
      "Speed: 364.0ms preprocess, 85.5ms inference, 18.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    }
   ],
   "source": [
    "DATA = PATHS['test']\n",
    "\n",
    "try:\n",
    "    model\n",
    "    print(\"Loaded trained model\")\n",
    "except:\n",
    "    # Load the best model\n",
    "    # List dir and find latest run\n",
    "    runs = listdir(f\"./logs\")\n",
    "    runs.sort()\n",
    "    while True:\n",
    "        latest_run = runs.pop()\n",
    "        if path.isdir(f\"./logs/{latest_run}\") and RECIPE_NAME in latest_run:\n",
    "            break\n",
    "    latest_run = \"all_run-20240614-0444198\"\n",
    "    modelPath = f\"./logs/{latest_run}/weights/best.pt\"\n",
    "    print(f\"Latest run: {latest_run}, loading model from {modelPath}\")\n",
    "    bestModel = YOLO(modelPath).to('cuda')\n",
    "    model = bestModel\n",
    "    print(\"Loaded best model\")\n",
    "\n",
    "while cv2.waitKey(0) != ord('q'):\n",
    "    cv2.destroyAllWindows()\n",
    "    randomFile = random.choice(listdir(DATA))\n",
    "    # randomFile = \"obb_resistor_yellow_violet_black_gold_brown_470E-1-1_30.png\"\n",
    "    print(f\"Random file: {randomFile}\")\n",
    "    results = model.predict(f\"{DATA}/{randomFile}\", show=True)\n",
    "    if len(results[0].obb.xyxyxyxy) > 0:\n",
    "        cropped_img = crop_image_to_bbox(results[0].orig_img, results[0].obb.xyxyxyxy)\n",
    "        cv2.imshow(\"Cropped Image\", cropped_img)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw vertices to verify direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random file: obb_resistor_brown_black_black_brown_brown_100E1-1_8.png\n",
      "\n",
      "image 1/1 c:\\Users\\Shaheen\\OneDrive - Imperial College London\\Uni\\CW Labs\\Year 4\\FYP\\src\\vision\\..\\..\\datasets\\full\\resistor\\imgs\\obb_resistor_brown_black_black_brown_brown_100E1-1_8.png: 480x640 198.5ms\n",
      "Speed: 42.0ms preprocess, 198.5ms inference, 45.5ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def draw_bounding_box(image, bbox, bbox_format, color=(0, 255, 0), thickness=2):\n",
    "    \"\"\"\n",
    "    Draws a bounding box on the image with an arrow indicating orientation for the xyxyxyxy format.\n",
    "\n",
    "    Args:\n",
    "    - image (np.array): The input image.\n",
    "    - bbox (tensor): The bounding box coordinates.\n",
    "    - bbox_format (str): The format of the bounding box ('xywhr', 'xyxy', 'xyxyxyxy', 'xyxyxyxyn').\n",
    "    - color (tuple): The color of the bounding box (default is green).\n",
    "    - thickness (int): The thickness of the bounding box lines (default is 2).\n",
    "\n",
    "    Returns:\n",
    "    - image_with_box (np.array): The image with the bounding box drawn.\n",
    "    \"\"\"\n",
    "    image_with_box = image.copy()  # Make a copy of the image to avoid overwriting\n",
    "\n",
    "    if bbox_format == 'xywhr':\n",
    "        # xywhr: [x_center, y_center, width, height, rotation (in radians)]\n",
    "        x_center, y_center, width, height, rotation = bbox[0]\n",
    "        center = (int(x_center.item()), int(y_center.item()))\n",
    "        size = (int(width.item()), int(height.item()))\n",
    "        angle = np.degrees(rotation.item())\n",
    "\n",
    "        rect = ((center[0], center[1]), (size[0], size[1]), angle)\n",
    "        box = cv2.boxPoints(rect)\n",
    "        box = np.int0(box)\n",
    "\n",
    "    elif bbox_format == 'xyxy':\n",
    "        # xyxy: [x_min, y_min, x_max, y_max]\n",
    "        x_min, y_min, x_max, y_max = bbox[0]\n",
    "        box = np.array([[x_min.item(), y_min.item()],\n",
    "                        [x_max.item(), y_min.item()],\n",
    "                        [x_max.item(), y_max.item()],\n",
    "                        [x_min.item(), y_max.item()]], dtype=np.int0)\n",
    "\n",
    "    elif bbox_format == 'xyxyxyxy':\n",
    "        # xyxyxyxy: [[x1, y1], [x2, y2], [x3, y3], [x4, y4]]\n",
    "        box = np.array(bbox[0].cpu(), dtype=np.int0)\n",
    "\n",
    "    elif bbox_format == 'xyxyxyxyn':\n",
    "        # xyxyxyxyn: normalized [[x1, y1], [x2, y2], [x3, y3], [x4, y4]]\n",
    "        h, w = image.shape[:2]\n",
    "        box = np.array(bbox[0].cpu() * np.array([w, h]), dtype=np.int0)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported bbox_format: {bbox_format}\")\n",
    "\n",
    "    # Draw the polygon\n",
    "    image_with_box = cv2.polylines(image_with_box, [box], isClosed=True, color=color, thickness=thickness)\n",
    "\n",
    "    # Draw an arrow for the xyxyxyxy format to indicate orientation\n",
    "    if bbox_format == 'xyxyxyxy':\n",
    "        start_point = (int(box[0][0]), int(box[0][1]))\n",
    "        end_point = (int(box[1][0]), int(box[1][1]))\n",
    "        image_with_box = cv2.arrowedLine(image_with_box, start_point, end_point, color, thickness, tipLength=0.3)\n",
    "\n",
    "     # Number each vertex\n",
    "    for i, (x, y) in enumerate(box):\n",
    "        cv2.putText(image_with_box, str(i+1), (x, y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), thickness, cv2.LINE_AA)\n",
    "\n",
    "    return image_with_box\n",
    "\n",
    "DATA = PATHS['resistors']\n",
    "randomFile = random.choice(listdir(DATA))\n",
    "print(f\"Random file: {randomFile}\")\n",
    "results = model.predict(f\"{DATA}/{randomFile}\", show=True)\n",
    "\n",
    "cv2.imshow(randomFile, draw_bounding_box(results[0].orig_img, results[0].obb.xyxyxyxy, 'xyxyxyxy'))\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export model to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'logs\\all_run-20240614-0444198\\weights\\best.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 16, 8400) (6.6 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.14.1 opset 13...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success  1.5s, saved as 'logs\\all_run-20240614-0444198\\weights\\best.onnx' (11.9 MB)\n",
      "\n",
      "Export complete (1.9s)\n",
      "Results saved to \u001b[1mC:\\Users\\Shaheen\\OneDrive - Imperial College London\\Uni\\CW Labs\\Year 4\\FYP\\src\\vision\\logs\\all_run-20240614-0444198\\weights\u001b[0m\n",
      "Predict:         yolo predict task=obb model=logs\\all_run-20240614-0444198\\weights\\best.onnx imgsz=640  \n",
      "Validate:        yolo val task=obb model=logs\\all_run-20240614-0444198\\weights\\best.onnx imgsz=640 data=./models/recipes/dataset_desc.yaml  \n",
      "Visualize:       https://netron.app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'logs\\\\all_run-20240614-0444198\\\\weights\\\\best.onnx'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.export(format=\"onnx\", opset=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random file: obb_ceramic_capacitor_472k_1.png\n",
      "Output shape: (1, 16, 8400)\n",
      "Filtered boxes: 0\n",
      "Random file: obb_resistor_green_brown_black_red_brown_510E2-1_1.png\n",
      "Output shape: (1, 16, 8400)\n",
      "Filtered boxes: 10\n",
      "Random file: obb_resistor_orange_blue_black_gold_brown_gold_360E-1-1_14.png\n",
      "Output shape: (1, 16, 8400)\n",
      "Filtered boxes: 12\n",
      "Random file: obb_ceramic_capacitor_10_6.png\n",
      "Output shape: (1, 16, 8400)\n",
      "Filtered boxes: 0\n",
      "Random file: obb_led_yellow_4.png\n",
      "Output shape: (1, 16, 8400)\n",
      "Filtered boxes: 0\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "from os import listdir\n",
    "\n",
    "\n",
    "# Create the ONNX inference session\n",
    "onnxModel = ort.InferenceSession(f\"./logs\\\\all_run-20240610-045510\\\\weights\\\\best.onnx\")\n",
    "CONFIDENCE_THRESHOLD = 0.1\n",
    "\n",
    "def preprocess(image):\n",
    "    input_shape = onnxModel.get_inputs()[0].shape  # get input shape\n",
    "    input_height, input_width = input_shape[2], input_shape[3]\n",
    "\n",
    "    image = cv2.resize(image, (input_width, input_height))\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # convert to RGB\n",
    "    image = image.astype(np.float32)\n",
    "    image = image / 255.0  # normalize to [0, 1]\n",
    "    image = np.transpose(image, (2, 0, 1))  # convert to CHW\n",
    "    image = np.expand_dims(image, axis=0)  # add batch dimension\n",
    "\n",
    "    return image\n",
    "\n",
    "def postprocess(output, original_image):\n",
    "    # Debugging: print the shape of the output\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "\n",
    "    output = np.squeeze(output)  # remove batch dimension\n",
    "\n",
    "    boxes = []\n",
    "    for i in range(output.shape[1]):\n",
    "        box = output[:, i]\n",
    "        x_center, y_center, width, height, objectness = box[:5]\n",
    "        if objectness > CONFIDENCE_THRESHOLD:\n",
    "            x_center *= original_image.shape[1]  # scale back to original image size\n",
    "            y_center *= original_image.shape[0]\n",
    "            width *= original_image.shape[1]\n",
    "            height *= original_image.shape[0]\n",
    "\n",
    "            x_min = int(x_center - width / 2)\n",
    "            y_min = int(y_center - height / 2)\n",
    "            x_max = int(x_center + width / 2)\n",
    "            y_max = int(y_center + height / 2)\n",
    "            boxes.append((x_min, y_min, x_max, y_max))\n",
    "    \n",
    "    print(f\"Filtered boxes: {len(boxes)}\")\n",
    "\n",
    "    for box in boxes:\n",
    "        cv2.rectangle(original_image, (box[0], box[1]), (box[2], box[3]), (0, 255, 0), 2)\n",
    "\n",
    "    return original_image\n",
    "\n",
    "# Fetch the actual input name from the model\n",
    "input_name = onnxModel.get_inputs()[0].name\n",
    "\n",
    "while cv2.waitKey(0) != ord('q'):\n",
    "    cv2.destroyAllWindows()\n",
    "    randomFile = random.choice(listdir(DATA))\n",
    "    print(f\"Random file: {randomFile}\")\n",
    "\n",
    "    image = cv2.imread(f\"{DATA}/{randomFile}\")\n",
    "    input_tensor = preprocess(image)\n",
    "\n",
    "    # Perform inference\n",
    "    results = onnxModel.run(None, {input_name: input_tensor})[0]\n",
    "    output_image = postprocess(results, image.copy())\n",
    "\n",
    "    cv2.imshow(randomFile, output_image)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[     5.2338      17.409      26.986 ...      510.89       548.1      579.53]\n",
      "  [     5.7821       5.787      7.7035 ...      599.64      588.09      583.31]\n",
      "  [     11.832       24.92      39.459 ...      238.35      186.63      154.18]\n",
      "  ...\n",
      "  [ 3.8743e-07  5.6624e-07  5.6624e-07 ...    4.53e-06  4.2915e-06  4.4405e-06]\n",
      "  [  1.809e-05  1.0133e-05  7.6294e-06 ...  1.4275e-05  1.7732e-05  3.8892e-05]\n",
      "  [  0.0060101    0.029564    0.015276 ...  -0.0082752  -0.0093197    0.017739]]]\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random file: obb_resistor_yellow_violet_black_gold_brown_470E-1-1_30.png\n",
      "\n",
      "image 1/1 c:\\Users\\Shaheen\\OneDrive - Imperial College London\\Uni\\CW Labs\\Year 4\\FYP\\src\\vision\\datasets\\full\\current\\images\\test\\obb_resistor_yellow_violet_black_gold_brown_470E-1-1_30.png: 480x640 47.5ms\n",
      "Speed: 3.5ms preprocess, 47.5ms inference, 7.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[377 153]\n"
     ]
    }
   ],
   "source": [
    "while cv2.waitKey(0) != ord('q'):\n",
    "    cv2.destroyAllWindows()\n",
    "    randomFile = random.choice(listdir(DATA))\n",
    "    randomFile = \"obb_resistor_yellow_violet_black_gold_brown_470E-1-1_30.png\"\n",
    "    print(f\"Random file: {randomFile}\")\n",
    "    results = model.predict(f\"{DATA}/{randomFile}\", show=True)\n",
    "    cropped_img = crop_image_to_bbox(results[0].orig_img, results[0].obb.xyxyxyxy)\n",
    "    cv2.imshow(\"Cropped Image\", cropped_img)\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
